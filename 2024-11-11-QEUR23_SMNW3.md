---
title: QEUR23_SMNW3 – Siamese Networkを使ってモデルを学習してみる(EMBEDDING生成)
date: 2024-11-11
tags: ["QEUシステム", "メトリックス", "Python言語", "Siamese Network", "NSOARTC", "データセット", "Embedding", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_SMNW3 – Siamese Networkを使ってモデルを学習してみる(EMBEDDING生成)

## ～ この手法の発展の方向は、いろいろある！ ～

QEU:FOUNDER ： “前回までで、Siamese Network（以下SN）モデルを学習するためのデータセットができました。いよいよ学習をしてみましょう。D先生は、今回のプロジェクトで予測精度がどれぐらい上がると思う？”

![imageSMN1-4-1](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-1.jpg)

D先生 ： “う～ん・・・。（現場が）安心して使うには、最低90％の正確度を達成して欲しいです。”

![imageSMN1-4-2](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-2.jpg)

QEU:FOUNDER ： “さて、どうかなあ・・・。実際には、肉眼でも判別しにくいケースがあるよ。なにはともあれ、プログラムをドン！！”

```python
# ---
#import os
os.environ["KERAS_BACKEND"] = "tensorflow"

# ---
import keras
import numpy as np
from keras import datasets

######################################
# DATASET LOADING
######################################
# ---
from datasets import load_dataset

# ---
# データセットの読み込み
dataset = load_dataset("YxBxRyXJx/SNWimages_train_1110")
#dataset['train'].features

# ---
split_dataset = dataset["train"].train_test_split(test_size=0.3, shuffle=True)
ds_train = split_dataset["train"]
ds_val = split_dataset["test"]
# ----
print("Train len: ", len(ds_train))
print("Val len: ", len(ds_val))

# ---
X_train = np.zeros([len(ds_train), 26, 36])
X_test = np.zeros([len(ds_val), 26, 36])
y_train = np.zeros(len(ds_train))
y_test = np.zeros(len(ds_val))

# ---
for i in range(len(ds_train)):
    X_train[i,:,:] = np.array(ds_train[i]['image'])
    y_train[i] = int(ds_train[i]['label'])

# ---
for i in range(len(ds_val)):
    X_test[i,:,:] = np.array(ds_val[i]['image'])
    y_test[i] = int(ds_val[i]['label'])

# ---
image_size = 36*26

# ---
# グラフ化
import matplotlib.pyplot as plt

for i in range(10):
    plt.subplot(1, 10, i + 1)
    plt.imshow(X_train[i], cmap="gray")
    plt.title(f'{y_train[i]}')
    plt.axis("off")

```

D先生 ： “それぞれの画像データとラベルが出てきました。このラベルの数字はピンアドレスでしたよね。”

![imageSMN1-4-3](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-3.jpg)

QEU:FOUNDER ： “そうです。ただし、Label=0は**合格品**で欠陥がありません。他のラベルは全部**欠陥品**です。それでは、抽出した2枚の画像でペアを組んで、モデル学習へのインプット情報を形成しましょう。”

```python
# ---
X_train = X_train.reshape(-1, image_size)
X_test = X_test.reshape(-1, image_size)
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

# ---
X_train = X_train / 255.0
X_test = X_test / 255.0

# ---
# 1回あたりのデータ生成処理。これができるからSiamese Networkはらくちん！
def one_turn_careation(i, X, y, X_pairs, y_pairs):

        digit = y[i]

        # 同じLabelを表すランダムな画像を見つけます。これは正のペアです。
        positive_digit_index = np.random.choice(np.where(y == digit)[0])

        # 画像のペアをリストに追加します。     
        X_pairs.append([X[i], X[positive_digit_index]])

        # これは正のペアなので、真のラベルは 0 です。同じLabelを表す画像間の距離は小さいはずなので、ラベルとして 0 を使用します。     
        y_pairs.append([0])

        # 異なるLabelを表すランダムな画像を見つけます。これは負のペアです。
        negative_digit_index = np.random.choice(np.where(y != digit)[0])

        # 画像のペアをリストに追加します。
        X_pairs.append([X[i], X[negative_digit_index]])

        # これは負のペアなので、真のラベルは 1 です。異なるLabelを表す画像間の距離は大きいはずなので、ラベルとして 1 を使用します。
        y_pairs.append([1])
        
        return X_pairs, y_pairs

# ---
#正と負の画像のペアを使用して、Siamese Network をトレーニングします。正のペアは同じ数字を表す 2 つの画像で構成され、負のペアは異なる数字を表す 2 つの画像で構成されます。
# ---
def generate_pairs(X, y):
    """
    指定された画像の配列から、ポジティブとネガティブの画像のペアのコレクションを作成します。
    正のペアには同じ数字の画像が 2 つ含まれます。負のペアには異なる数字を表す 2 つの画像が含まれます。
    """
    X_pairs = []
    y_pairs = []

    # 循環生成
    for i in range(len(X)):
        for j in range(10):     
            # one turn
            X_pairs, y_pairs = one_turn_careation(i, X, y, X_pairs, y_pairs)
        
    # ---    
    indices = np.arange(len(X_pairs))
    np.random.shuffle(indices)

    return np.array(X_pairs)[indices], np.array(y_pairs)[indices]

# ---
# モデルのトレーニングとテスト用のペアを生成しましょう。
# ---
X_train_pairs, y_train_pairs = generate_pairs(X_train, y_train)
X_test_pairs, y_test_pairs = generate_pairs(X_test, y_test)
print("X_train_pairs shape:", X_train_pairs.shape)
print("X_test_pairs shape:", X_test_pairs.shape)

# ---
# 正しく生成されたことを確認するために、これらのペアのいくつかを表示してみましょう。
# ---
def display_pairs(X, y, correct=None):
    """
    Displays the first ten pairs from the supplied array.

    Args:
        - X: An array containing the image pairs.
        - y: An array containing the corresponding label (0 if both
            pairs are the same, and 1 if both pairs are different.)
        - correct (optional): An array of boolean values indicating whether
            the supplied labels correctly represent the image pairs.
    """

    n = 10

    plt.figure(figsize=(20, 6))
    for i, (image1, image2) in enumerate(zip(X[:n, 0], X[:n, 1])):
        label = int(y[:n][i][0])

        text = "Positive" if label == 0 else "Negative"
        color = "silver"

        # If we know whether the supplied labels are correct, let's change the
        # text and the face color of the annotation on the chart.
        if correct is not None:
            text = "Same" if label == 0 else "Different"
            color = "mediumseagreen" if correct[:n][i][0] else "indianred"

        ax = plt.subplot(3, n, i + 1)
        ax.text(
            1,
            -3,
            text,
            style="italic",
            bbox={"facecolor": color, "pad": 4},
        )

        plt.imshow(image1.reshape(26, 36), cmap="gray")
        plt.axis("off")

        ax = plt.subplot(3, n, i + 1 + n)
        plt.imshow(image2.reshape(26, 36), cmap="gray")
        plt.axis("off")

    plt.show()

# ---
display_pairs(X_train_pairs, y_train_pairs)

```

C部長： “ここまでで、画像ペア情報が生成されるんですね。”

![imageSMN1-4-4](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-4.jpg)

QEU:FOUNDER ： “同じラベルの画像がペアになると「Positive」に、違うラベルがペアになると「Negative」になります。さて、これから学習するモデルを生成しましょう。”

```python
# ---
from keras import Input
from keras.layers import Dense, Lambda
from keras.ops import norm
from keras.models import Sequential, Model
from keras.saving import register_keras_serializable
import tensorflow as tf
from tensorflow.keras import backend as K

# Register the custom function for serialization
@register_keras_serializable()
def euclidean_distance(twins):
    """Compute the euclidean distance (norm) of the output of
    the twin networks.
    """
    twin1_output, twin2_output = twins
    return tf.sqrt(tf.reduce_sum(tf.square(twin1_output - twin2_output), axis=1, keepdims=True))

# Define inputs
input1 = Input(shape=(image_size,))
input2 = Input(shape=(image_size,))

# Define the network
network = Sequential(
    [
        Input(shape=(image_size,)),
        Dense(512, activation="relu"),
        Dense(256, activation="relu"),
        Dense(128, activation=None),
    ]
)

# Create twin networks
twin1 = network(input1)
twin2 = network(input2)

# Compute distance using Lambda layer with output_shape specified
distance = Lambda(euclidean_distance, output_shape=(1,))([twin1, twin2])

# Define the model
model = Model(inputs=[input1, input2], outputs=distance)

# Define and register contrastive loss function for serialization
@register_keras_serializable()
def contrastive_loss(y, d):
    margin = 1
    y = tf.cast(y, d.dtype)
    loss = (1 - y) / 2 * K.square(d) + y / 2 * K.square(K.maximum(0.0, margin - d))
    return loss

# Compile the model using contrastive loss
model.compile(loss=contrastive_loss, optimizer="adam", metrics=['binary_accuracy'])

# ---
# Display the model's architecture
model.summary()

```

D先生 ： “以前も確認しましたが、シンプルな構造のモデルですね。”

![imageSMN1-4-5](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-5.jpg)

QEU:FOUNDER ： “このモデルを学習しましょう。いよいよ学習指示の命令をドン！！”

```python
# ---
# Fit the model (assuming X_train_pairs and y_train_pairs are defined)
history = model.fit(
    x=[X_train_pairs[:, 0], X_train_pairs[:, 1]],
    y=y_train_pairs[:],
    validation_data=([X_test_pairs[:, 0], X_test_pairs[:, 1]], y_test_pairs[:]),
    batch_size=32,
    epochs=10,
)

# ---
# トレーニング中のトレーニング損失と検証損失をプロットできます。
# ---
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("Training and Validation Loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "val"], loc="upper right")
plt.show()

```

C部長： “ひさびさにKerasの学習プロセスをみました。Kerasの場合にはPytorchと違って、詳細を特に指定しなくても自動的に学習結果がでてきて楽ですね。”

![imageSMN1-4-6](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-6.jpg)

QEU:FOUNDER ： “これをグラフにするとこう（↓）なります。”

![imageSMN1-4-7](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-7.jpg)

D先生 ： “Train損失は減りますが、Validationロスが減らないですね。”

QEU:FOUNDER ： “つまり、**「これ以上は、なんともならん」**のです。ちなみに、学習データの量を変えても同じ感じです。次に予測をしてみましょう。”

```python
# ---
predictions = model.predict([X_test_pairs[0:10, 0], X_test_pairs[0:10, 1]])
print(predictions)

```

D先生 ： “なるほど。SNモデルの予測って、こんな感じで行われるのか・・・。予測値が0になると、2つの画像のラベルが同じであり・・・。”

![imageSMN1-4-8](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-8.jpg)

QEU:FOUNDER ： “予測値が大きく（値が１に近く）なると、違う属性の画像なんでしょうね。それでは、これを加味して予測精度を評価すると、以下のようになります。”

![imageSMN1-4-9](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-9.jpg)

C部長： “あれ？こんなに能力が低いのか・・・。これでは、本当に現場で実用になるのかなあ・・・。”

```python
# ---
# シーケンシャル モデルを参照して、新しい画像のEMBEDDINGを生成することができます。
embedding_model = model.layers[2]

# 合格品を表す画像を2つランダムに選択し、不良品を表す画像を1つ選択してみましょう。:
# [0, 1, 2, 3, 6, 7, 8, 9, 12, 13, 15, 16, 17, 18, 19, 20, 21, 24, 25]
digits = np.where(y_test == 0)[0]
index1 = np.random.choice(digits)
index2 = np.random.choice(digits)
index3 = np.random.choice(np.where(y_test == 1)[0])
###
#index1, index2, index3

# ---
# 次の 3 つの画像のEMBEDDINGを生成できます。
embedding1 = embedding_model.predict(X_test[index1].reshape(1, -1))
embedding2 = embedding_model.predict(X_test[index2].reshape(1, -1))
embedding3 = embedding_model.predict(X_test[index3].reshape(1, -1))
###
embedding1.shape
#(1, 128)

```

QEU:FOUNDER ： “ここで私たちが気を付けなければならないのは、この精度（正確度）の定義は、**「2つの画像の一致性」であること**です。C部長のように外観検査自動機を使いたい人の場合には、欲しい定義とは**「合格品と不合格品を見分ける能力」**のことでしょ？さて、これから、いよいよEmbeddingの生成になります。”

![imageSMN1-4-10](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-10.jpg)

C部長： “ベクトルがバラバラとでてきました。これが、Embeddingなのか・・・。”

D先生 ： “今回の最も大きな成果は、画像からEmbeddingを生成することが可能になったことです。このEmbeddingをつかって予測をしてみましょう。”

QEU:FOUNDER ： “以下では、normを使っていますが、べつに**ユーグリッド距離などを使っても同じ**ですからね。”

![imageSMN1-4-11](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-11.jpg)

D先生 ： “コレを見るとEmbeddingの有用性がわかるでしょ？”

C部長 ： “なるほど、Embeddingによる予測の方が実用的になるかもしれません。なにしろ、**SNモデルを通して「製品品質」を抽出した**わけですね。”

QEU:FOUNDER ： “今回が「Siamese Networkによる外観検査」プロジェクトの山場でした。めでたく大成功のようですね。次は、ウィニングラン（延長戦）としてEmbeddingを使ってあそんでみましょう。”

## ～ まとめ ～

QEU:FOUNDER ： “Siamese Networkプロジェクトも大成功に終わったし、ひさびさに「まとめ」として技術的な話をしましょう。本来は、「まとめ」は技術的な総括の場なのです。”

C部長 : “床屋政談の場だとおもっていました（笑）。”

![imageSMN1-4-12](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-12.jpg)

QEU:FOUNDER ： “Siamese Networkという手法は、ViTモデルによる画像判別ほどの高い能力をもっていません。これをなぜ、我々が「イチオシ」しているのか・・・。”

![imageSMN1-4-13](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-13.jpg)

C部長 : “Raspberry Piレベルでも予測可能な、簡単な方法だからでしょ？”

QEU:FOUNDER ： “まあ、それは一つあるよね。この方法はRaspberry Pi4であれば十分に、現場で使えるでしょう。多分、検査に要するタクトタイムは10秒もないと思います。これ（簡単な検査）が、小生の本当の目的ではありません。”

C部長 : “それが現実的、かつノーマルな考え方だと思うんですが・・・。”

![imageSMN1-4-14](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-14.jpg)

QEU:FOUNDER ： “今回のSiamese Networkの社会的インパクトのポイントは、**Embeddingの生成**です。品質というのは、「モノの抽象化」でしょ？ある角度から見た・・・。これを、統計を使わずに、一品単位で、多次元ベクトルの形で実現したわけです。このように品質の抽象化を実現すると、いろいろな局面で使えます。この考え方が、DMAICSサイクルのもう一つの側面です。”

![imageSMN1-4-15](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-15.jpg)

C部長 : “もうPDCAサイクルは時代遅れです。**DMAICSサイクルでないと、モノの価値が上がりません**。”

![imageSMN1-4-16](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-16.jpg)

QEU:FOUNDER ： “バラツキを減らす程度であればPDCAでも可能だが、価値を上げることはできません。ここまで追い込まれがJ国（↑）、もうそろそろ考え方をかえないと。すでに何回も言っているが・・・。”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

C部長 : “そもそも、PDCAサイクルの行きつく先が**「皆さん、テレビを見てください」**ですからね。それにしても、彼らは変わらないですよね。頑固なのか？”

![imageSMN1-4-17](/2024-11-11-QEUR23_SMNW3/imageSMN1-4-17.jpg)

QEU:FOUNDER ： “古いんですよ。もう考え方が60年代、デミングから全く進歩していない・・・。そして、その遅れた考え方を**世に押し売りして飯を食っている**人がいる。”

C部長 : “それが**J国の失われた30年**の根本問題です。”
