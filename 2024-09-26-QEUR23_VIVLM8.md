---
title: QEUR23_VIVLM8:  もっとも簡単なファインチューニング（正方形）
date: 2024-09-26
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "Vision language Model"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VIVLM8:  もっとも簡単なファインチューニング（正方形）

## ～ 失敗は、成功の元・・・。 ～

QEU:FOUNDER ： “さて、学習用のデータの準備が完了したので、いよいよファインチューニングに行きます。”

![imageVLM1-9-1](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-1.jpg)

D先生 ： “Huggingfaceにアップロードされているので、今後の展開が楽になります。”

QEU:FOUNDER ： “じゃあ、これからプログラムを動かしましょう。Phi-3-visionを学習するには、**A100程度のGPUが必要になります**。”

```python
# ----
!pip install flash_attn Requests torchvision transformers accelerate peft
!pip install -U datasets wandb evaluate sacrebleu rouge_score

# ---
import os
import numpy as np
import pandas as pd

import base64
import requests
from PIL import Image
from io import BytesIO

import transformers
from peft import LoraConfig, get_peft_model, PeftModel

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision.transforms.functional import resize, to_pil_image
from torchvision import transforms

from datasets import load_dataset
import evaluate

import matplotlib.pyplot as plt
from textwrap import wrap

torch.manual_seed(3)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

dtype=torch.bfloat16

batch_size = 1
base_model_id = "microsoft/Phi-3-vision-128k-instruct"
model_dir = "drive/MyDrive/FTmodels/peft_tanshi"
     
# ---
#2. Load Dataset
raw_train_dataset = load_dataset("YxBxRyXJx/VMLimages_test_0924") 
raw_test_dataset = load_dataset("YxBxRyXJx/VMLimages_0924") 
print(raw_train_dataset)

# ---
#3. Load Model and Processor
model = transformers.AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=dtype,
    device_map="auto",
    trust_remote_code=True,
)

processor = transformers.AutoProcessor.from_pretrained(base_model_id, trust_remote_code=True)
model = model.to(device)
print(model)

```

D先生 ： “まずは、こんな結果がでました。”

![imageVLM1-9-2](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-2.jpg)

QEU:FOUNDER ： “とくに意味がないんだけどね・・・（笑）。じゃあ、つづきましょう。”

```python
# ---
# 2. no need filtered
filtered_train_dataset = raw_train_dataset
filtered_test_dataset = raw_test_dataset

# ---
# 4. Inference with Base Model
# ---
id = 1
question = filtered_test_dataset["train"][id]['question']
image = filtered_test_dataset["train"][id]["image"].convert("RGB")
description = filtered_test_dataset["train"][id]['answer']
#print(f"CHAT_TEMPLATE: \n{processor.tokenizer.chat_template}")

############################
# プロンプトの設定
############################
# ---
userPrompt = question
messages = [
    {"role": "user", "content": f"<|image_1|>\n{userPrompt}"}
]
prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
print(f"\nPROMPT: \n{prompt}")

############################
# テスト推論する
############################
# ---
inputs = processor(prompt, [image], return_tensors="pt").to(device)
inputs['cache_position'] = None  # ここでcache_positionを追加

generation_args = {
    "max_new_tokens": 512,
    "temperature": 0.1,
    "do_sample": True  # do_sampleをTrueに設定
}

generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)
generate_ids = generate_ids[:, inputs["input_ids"].shape[1]:]
response = processor.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]

plt.imshow(image)
plt.axis("off")
plt.show()
# ---
print(f"\nIMAGE DESCRIPTION:\n{description}")
print(f"\nMODEL_RESPONSE: \n{response}")

```

D先生 ： “これは、単純な推論をやってみた結果です。pre-trainなので、うまく検査できないのは、あたりまえ・・・。プログラムを見ると、**「filtered_train_dataset」**なんかは必要なんですか？”

![imageVLM1-9-3](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-3.jpg)

QEU:FOUNDER ： “現在は必要ではないんですが、今後、学習データが多くなったら、あるていどの**「間引き」が必要になる**と思っています。プロビジョンですね。それでは、さらにつづきます。”

```python
# ---
# 5. Fine-tuning
# 5.1 Prepare Dataset
split_dataset = filtered_train_dataset["train"].train_test_split(test_size=0.2, seed=42)

train_dataset = split_dataset["train"]
val_dataset = split_dataset["test"]
test_dataset = filtered_test_dataset["train"]

columns_to_keep = ["image", "question", "answer"]

train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col not in columns_to_keep])
val_dataset = val_dataset.remove_columns([col for col in val_dataset.column_names if col not in col-umns_to_keep])
test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names if col not in columns_to_keep])

# ---
def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 30))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")

sample_images_to_visualize = [np.array(train_dataset[i]["image"]) for i in range(5)]
sample_captions = [train_dataset[i]['answer'] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)

```

QEU:FOUNDER ： “これ（↓）が、データ中のレコードの構造です。イメージとコメントが見やすくなるようにまとめました。”

![imageVLM1-9-4](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-4.jpg)

D先生 ： “わかりやすいデータのまとめですね。次に行ってください。”

```python
# ---
# 5. Fine-tuning
# 5.1 Prepare Dataset
split_dataset = filtered_train_dataset["train"].train_test_split(test_size=0.2, seed=42)

train_dataset = split_dataset["train"]
val_dataset = split_dataset["test"]
test_dataset = filtered_test_dataset["train"]

columns_to_keep = ["image", "question", "answer"]

train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col not in columns_to_keep])
val_dataset = val_dataset.remove_columns([col for col in val_dataset.column_names if col not in col-umns_to_keep])
test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names if col not in columns_to_keep])

# ---
def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 30))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")

sample_images_to_visualize = [np.array(train_dataset[i]["image"]) for i in range(5)]
sample_captions = [train_dataset[i]['answer'] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)

# ---
# 5.2 Setup DataCollator
class DataCollator:
    def __init__(self, processor):
        self.processor = processor

    def __call__(self, examples):
        example = examples[0]
        image = example["image"]
        user_prompt = example['question']
        answer = example['answer']
        # ---
        messages = [
            {"role": "user", "content": f"<|image_1|>\n{user_prompt}"}
        ]

        prompt = self.processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        answer = f"{answer}<|end|>\n<|endoftext|>"

        # Mask user_prompts for labels
        batch = self.processor(prompt, [image], return_tensors="pt")
        prompt_input_ids = batch["input_ids"]

        answer_input_ids = self.processor.tokenizer(answer, add_special_tokens=False, re-turn_tensors="pt")["input_ids"]

        concatenated_input_ids = torch.cat([prompt_input_ids, answer_input_ids], dim=1)
        ignore_index = -100
        labels = torch.cat(
            [
                torch.tensor([ignore_index] * len(prompt_input_ids[0])).unsqueeze(0),
                answer_input_ids,
            ],
            dim=1,
        )

        batch["input_ids"] = concatenated_input_ids
        batch["labels"] = labels

        # Ensure only floating-point tensors require gradients
        for key, value in batch.items():
            if isinstance(value, torch.Tensor) and torch.is_floating_point(value):
                batch[key] = value.clone().detach().requires_grad_(True)

        return batch

data_collator = DataCollator(processor)

# ---
# 5.3 Setup LoRA
model.resize_token_embeddings(len(processor.tokenizer))
model.gradient_checkpointing_enable()
#print(model.state_dict().keys())

# ---
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[
        "self_attn.q_proj.weight",
        "self_attn.k_proj.weight",
        "self_attn.v_proj.weight",
        "self_attn.qkv_proj.weight",
        "self_attn.out_proj.weight",
        "mlp.gate_up_proj",
        "mlp.down_proj"
    ],
    lora_dropout=0.06,
    bias="none",
    task_type="CAUSAL_LM",
    use_dora=False
)

peft_model = get_peft_model(model, lora_config)

train_dataset.start_iteration = 0
     
def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()

    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * traina-ble_params / all_param}"
    )

print_trainable_parameters(model)

```

QEU:FOUNDER ： “今回のトレーニングの規模が出てきました。”

![imageVLM1-9-5](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-5.jpg)

D先生 ： “へえ、意外・・・・。0.4％もトレーニングするのか？つづきをお願いします。”

```python
# ---
# 5.4 Training
training_args = transformers.TrainingArguments(
    num_train_epochs=4,                          # Number of training epochs
    per_device_train_batch_size=batch_size,      # Batch size for training
    per_device_eval_batch_size=batch_size,       # Batch size for evaluation
    gradient_accumulation_steps=6,               # Number of steps to accumulate gradients before updat-ing
    gradient_checkpointing=True,                 # Enable gradient checkpointing to save memory
    do_eval=True,                                # Perform evaluation during training
    save_total_limit=2,                          # Limit the total number of saved checkpoints
    evaluation_strategy="steps",                 # Evaluation strategy to use (here, at each specified number of steps)
    save_strategy="steps",                       # Save checkpoints at each specified number of steps
    save_steps=10,                               # Number of steps between each checkpoint save
    eval_steps=10,                               # Number of steps between each evaluation
    max_grad_norm=1,                             # Maximum gradient norm for clipping
    warmup_ratio=0.1,                            # Warmup ratio for learning rate schedule
    weight_decay=0.01,                           # Regularization technique to prevent overfitting
    # fp16=True,                                 # Enable mixed precision training with fp16 (enable it if Ampere architecture is unavailable)
    bf16=True,                                   # Enable mixed precision training with bf16
    logging_steps=10,                            # Number of steps between each log
    output_dir="outputs",                        # Directory to save the model outputs and checkpoints
    optim="adamw_torch",                         # Optimizer to use (AdamW with PyTorch)
    learning_rate=1e-4,                          # Learning rate for the optimizer
    lr_scheduler_type="constant",                # Learning rate scheduler type
    load_best_model_at_end=True,                 # Load the best model found during training at the end
    push_to_hub=False,                           # Whether to push the model to Hugging Face Hub
    run_name="phi-3-vision-finetuning-0925",          # Name of the run for experiment tracking
    report_to="wandb"                            # For experiment tracking (login to Weights & Biases needed)
)

# ---
class CustomTrainer(transformers.Trainer):
    def get_train_dataloader(self):
        # Ensure the DataLoader uses your custom DataCollator
        return DataLoader(
            self.train_dataset,
            batch_size=self.args.train_batch_size,
            collate_fn=self.data_collator,
            drop_last=self.args.dataloader_drop_last,
            num_workers=self.args.dataloader_num_workers,
        )

    def get_eval_dataloader(self, eval_dataset=None):
        # Ensure the DataLoader uses your custom DataCollator for evaluation
        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
        return DataLoader(
            eval_dataset,
            batch_size=self.args.eval_batch_size,
            collate_fn=self.data_collator,
            drop_last=self.args.dataloader_drop_last,
            num_workers=self.args.dataloader_num_workers,
        )

    def compute_loss(self, model, inputs, return_outputs=False):
        outputs = model(**inputs)
        loss = outputs.loss if isinstance(outputs, dict) else outputs[0]
        return (loss, outputs) if return_outputs else loss

# Ensure the model is in training mode
peft_model.train()

trainer = CustomTrainer(
    model=peft_model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    #callbacks=[early_stopping]
)

peft_model.config.use_cache = True

#---
trainer.train()
```

QEU:FOUNDER ： “損失値を見ると、意外に、うまくトレーニングされています。”


      (imageVLM1-9-6)
![imageVLM1-9-1](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-1.jpg)

D先生 ： “コレを使ってファインチューニングモデルでの推論をするのが楽しみです。”

QEU:FOUNDER ： “じゃあ、これからプログラムを準備して推論をやってみましょう。”

```python
# ---
# 6. Inference with Finetuned Model
def load_with_peft_adapter(base_model_id, model_path, device="cpu", dtype=torch.bfloat16):
    """
        load model via Huggingface AutoModelForCausalLM and AutoProcessor
    """
    torch.set_default_device(device)

    with torch.device(device):

        processor = transformers.AutoProcessor.from_pretrained(model_path, trust_remote_code=True)

        model = transformers.AutoModelForCausalLM.from_pretrained(
            base_model_id,
            device_map=device,
            torch_dtype=dtype,
            trust_remote_code=True
        ).eval()

        model.resize_token_embeddings(len(processor.tokenizer))

        finetuned_model = PeftModel.from_pretrained(
            model,
            model_path
        ).eval()
        finetuned_model.to(device)
        finetuned_model.config.use_cache = True

        return finetuned_model, processor

finetuned_model, processor = load_with_peft_adapter(base_model_id, model_dir, device="cuda", dtype=dtype)


def inference_with_finetuned_model(finetuned_model, processor, tokenizer, image, device="cuda", temperature = 0.0):
  userPrompt = "Referenced image has two images inside. Compare the two images and find the dif-ferences. The two images have coordinate letters 1 to 9 horizontally and U, C, D vertically to find po-sition. You should compare the condition of the cubes at each position one by one. The top image is one of passed product. You should evaluate whether the product in the bottom image is passed or de-fective. If you find differences in the bottom image, explain its location and defect name."

  prompt = f"<|user|>\n<|image_1|>\n{userPrompt}<|end|>\n<|assistant|>\n"

  print(f"\nPROMPT: \n{prompt}")

  plt.imshow(image)
  plt.axis("off")
  plt.show()

  inputs = processor(prompt, [image], return_tensors="pt").to("cuda:0")
  inputs['cache_position'] = None  # ここでcache_positionを追加

  generation_args = {
      "max_new_tokens": 128,
      "temperature": 0.2,
      "do_sample": True  # do_sampleをTrueに設定
  }

  with torch.no_grad():
    generate_ids = finetuned_model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)

  generate_ids = generate_ids[:, inputs["input_ids"].shape[1]:]

  decoded_output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
  response = decoded_output[0]

  return response

# ---
id = 1
image = filtered_test_dataset["train"][id]["image"].convert("RGB")
description = filtered_test_dataset["train"][id]['answer']

response = inference_with_finetuned_model(finetuned_model, processor, processor.tokenizer, image, device)
# ---
print("-----------")
print(f"MODEL RESPONSE: {response}")
print("-----------")
print(f"EXPECTED RESPONSE: {description}")

```

D先生 ： “推論実行！！わくわく・・・・。”

![imageVLM1-9-7](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-7.jpg)

D先生 ： “残念、失敗かあ・・・・。”

QEU:FOUNDER ： “でも、ファインチューニングを通じて**コメントの型式が標準化されている**でしょ？”

D先生 ： “もっとたくさんの結果を見たいです。”

```python
# ---
# 7. Testing
def inference_with_base_model(id, dataset):
  image = dataset[id]["image"].convert("RGB")

  userPrompt = "Referenced image has two images inside. Compare the two images and find the differ-ences. The two images have coordinate letters 1 to 9 horizontally and U, C, D vertically to find position. You should compare the condition of the cubes at each position one by one. The top image is one of passed product. You should evaluate whether the product in the bottom image is passed or defective. If you find differences in the bottom image, explain its location and defect name."
  messages = [
      {"role": "user", "content": f"<|image_1|>\n{userPrompt}"}
  ]

  prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

  inputs = processor(prompt, [image], return_tensors="pt").to(device)
  inputs['cache_position'] = None  # ここでcache_positionを追加

  generation_args = {
      "max_new_tokens": 512,
      "temperature": 0.3,
      "do_sample": True  # do_sampleをTrueに設定
  }

  generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)

  generate_ids = generate_ids[:, inputs["input_ids"].shape[1]:]
  response = processor.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]

  return response


def inference_with_finetuned_model(id, dataset, finetuned_model, processor, tokenizer, device="cuda", temperature = 0.0):
  image = dataset[id]["image"].convert("RGB")

  userPrompt = "Referenced image has two images inside. Compare the two images and find the differ-ences. The two images have coordinate letters 1 to 9 horizontally and U, C, D vertically to find position. You should compare the condition of the cubes at each position one by one. The top image is one of passed product. You should evaluate whether the product in the bottom image is passed or defective. If you find differences in the bottom image, explain its location and defect name."

  prompt = f"<|user|>\n<|image_1|>\n{userPrompt}<|end|>\n<|assistant|>\n"

  inputs = processor(prompt, [image], return_tensors="pt").to("cuda:0")
  inputs['cache_position'] = None  # ここでcache_positionを追加

  generation_args = {
      "max_new_tokens": 128,
      "temperature": 0.3,
      "do_sample": True  # do_sampleをTrueに設定
  }

  with torch.no_grad():
    generate_ids = finetuned_model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)

  generate_ids = generate_ids[:, inputs["input_ids"].shape[1]:]

  decoded_output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
  response = decoded_output[0]

  return response
     
images_to_visualize = [np.array(test_dataset[i]["image"]) for i in [10,20,30,40,50]] #len(test_dataset)
expected_captions = [test_dataset[i]['answer'] for i in [10,20,30,40,50]]
base_model_captions = [inference_with_base_model(i, test_dataset) for i in [10,20,30,40,50]]
finetuned_model_captions = [inference_with_finetuned_model(i, test_dataset, finetuned_model, processor, processor.tokenizer) for i in [10,20,30,40,50]]
     
# ---
def plot_images(images, expected_captions, model_captions, model_captions_color="red"):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)

        expected_caption = expected_captions[i]
        expected_caption = "\n".join(wrap(expected_caption, 30))

        model_caption = model_captions[i]
        model_caption = "\n".join(wrap(model_caption, 30))

        plt.title(expected_caption, color="green", fontsize=12)
        plt.xlabel(model_caption, color=model_captions_color, fontsize=10, labelpad=20)

        plt.imshow(images[i])
        ax.set_xticks([])
        ax.set_yticks([])

    plt.tight_layout()
    plt.show()

plot_images(images_to_visualize, expected_captions, base_model_captions)

```

QEU:FOUNDER ： “まとめて実行してみましたよ。”

![imageVLM1-9-8](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-8.jpg)


D先生 ： “う～ん・・・・。全然、**「あたり（正解）」がない**ですね。”

QEU:FOUNDER ： “それでも、コメントの標準化など、モデルに少しだけ進歩があったでしょ？”

D先生 ： “いや・・・。大いに進歩がありました。学習データの追加が必要ですね。”

QEU:FOUNDER ： “でも、計算資源制約により、trainデータのレコード数は1000以下、testデータのレコード数は400以下にしますよ。”

D先生 ： “今後の進め方の枠組みが決まりました。あとは、追加データの準備と学習、評価の繰り返しですね。”


## ～ まとめ ～

## ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “手元に文献が見当たらないので、この本（↑）かどうかわからないが、何かの本を読んだ記憶を元に、文脈だけを説明します。たしか、**「ある部門が仕事にどれだけエネルギーを投入（努力）しても、その結果がゼロであれば、その投入エネルギーをゼロとみなす」**という考え方です。”

![imageVLM1-9-9](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-9.jpg)

C部長 : “その考え方には**「時間の観念」**が抜けています。スロッシングの事例では、あることがトリガーとなって、後程大きな成果がでてきます。”

![imageVLM1-9-10](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-10.jpg)

QEU:FOUNDER ： “さらに**「空間の概念」**もぬけています。バタフライ効果によると、エネルギー（努力）を共有することにより、その努力が別の部門で成果として現れるかもしれない。それが、QEUシステムのDMAICSモデルです。もし、リニア的な部門評価制度が普遍的に有用だとすると、G社の20％制度は現れないでしょ？”

[![MOVIE2](http://img.youtube.com/vi/twEP9eHgP9c/0.jpg)](http://www.youtube.com/watch?v=twEP9eHgP9c "【グーグル日本法人元社長辻野氏と語る「AIのリスク」と「“安倍政治の負の遺産”のリスク」】郷原信郎の「日本の権力を斬る！」")

QEU:FOUNDER ： “だから、小生は**「平成時代はリニアの時代だった。そして、タグチメソッド(TM)はリニア発想のごり押しの象徴」**と思ってるんです。これが、小生がTMを勉強して得た最大の成果かな・・・。ちょっと悲しいが・・・。”

C部長 : “世の中にあまりある非線形のプロセスの最適化には、レガシーのパラメタ設計は使えないんですか。”

![imageVLM1-9-11](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-11.jpg)

QEU:FOUNDER ： “パラメタ設計？そもそもが使いにくいじゃない・・・。**「当たるも八卦、当たらぬも八卦」の手法**だから、小生には、君の質問への回答を持っていません。CS-T(T法)と総合SN比でやれば、少しだけ良くなるんです。ただし、その使用方法が**「DMAICS循環（↑）」**になります。レガシー法のように、**「一発バーン！！どうだ！!オレ様すごいだろ!!」**という景気の良いシナリオになりません。そもそも、T法はリニア前提です。実験水準の刻みを小さくしないとリニアにならないでしょ？”

![imageVLM1-9-12](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-12.jpg)

QEU:FOUNDER ： “ポイントは、総合SN比になります。標準値（ベースライン）が**ディープラーニング・モデルの予測値**です。だから、非線形だろうと、名義変量だろうと、何にでも適応できます。DMAICSサイクルを通じて、このディープラーニングのモデルを育てる必要があります。”

![imageVLM1-9-13](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-13.jpg)

QEU:FOUNDER ： “これが、最新版のDMAICSサイクルです。非線形に対応するためのモデルを育てることが、コンセプトの中心にあります。”

C部長 : “「シェア(Share)」の段階が、ピンとこないです。”

![imageVLM1-9-14](/2024-09-26-QEUR23_VIVLM8/imageVLM1-9-14.jpg)

QEU:FOUNDER ： “DMAICSサイクルは**「モデルを育成する」ことが改善根拠のベースにあります**。もし、類似したモデルが他にあれば、それを共有してモデルの精度を上げることは簡単にできます。**非線形なメソッドの改善スキームは、過去のデータを捨てることはないんです**。そして、この「CST+総合SN比」スキーム全体として、REINFORCEによる強化学習に似たモデル育成シナリオに近くなります。”

C部長 : “REINFORCEって、なつかしいなあ・・・。ロジックが難しいように見える反面、計算が軽いんですよねえ。”

QEU:FOUNDER ： “最新のLLMモデルの学習でも、強化学習としてREINFORCEは使われますよ。重要な点は、総合SN比による制御因子の選定は、レガシーのように「Larger is better」のように簡単じゃないことがあります。”

C部長 : “まあ、非線形ではSN比に加法性はありえないですけど・・・。”

QEU:FOUNDER ： “ベースラインのモデルが随時変わってくるので、SN比の自体の意味合いが大きく変わってくるんですよ。「誤差因子による出力のバラツキ」、「グラフの線形性」などをにらんで、定性的に評価するしかないですね。当面はね・・・。”

