---
title: QEUR23_VIVLM36 – 閑話休題～UnslothのVLMモデルを使ってLora学習する
date: 2024-10-25
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "Vision language Model"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VIVLM36 – 閑話休題～UnslothのVLMモデルを使ってLora学習する

## ～ 本当の「閑話休題」です！ ～

QEU:FOUNDER ： “さっき調べものをしていたところ、**Llama3.2-11B-visionのファインチューニング**の例が見つかったよ！！”

![imageVLM1-36-1](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-1.jpg)

D先生 ： “えっ！とうとう、以前から指摘されてきた課題が解決したんですか？”

QEU:FOUNDER ： “ここのスニペットでエラーが起こりました・・・（苦笑）。”

```python
import torch
from transformers import AutoProcessor
from unsloth import FastLanguageModel

model_id = "meta-llama/Llama-3.2-11B-Vision-Instruct"

# Initialize the model with Unsloth
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_id,
    max_seq_length=2048,
    dtype=torch.bfloat16,
    load_in_4bit=True,
)

processor = AutoProcessor.from_pretrained(model_id)

```

D先生 ： “以前と同じ部分でトラブっていますね。**FastLanguageModelというクラスはVLMに対応していない**んです。骨折り損（のくたびれ儲け）ですね。”

![imageVLM1-36-2](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-2.jpg)

QEU:FOUNDER ： “小生はアマノジャクだから、逆にファイトが沸いてきました。**前回はUnSlothのモデルを用いて推論をしました。今回は、学習をやってみましょう。**”

**（推論の出力）**

![imageVLM1-36-3](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-3.jpg)

**（推論の出力）**

![imageVLM1-36-4](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-4.jpg)

D先生 ： “ああ、あれですか・・・。Unslothのモデルは4bitで軽いので、このアプローチは実用的だと思いますよ。それにしても、さらにファインチューニング（学習）までいくとねえ・・・。”

QEU:FOUNDER ： “そんなもん、だめでもともと！やってみましょう。プログラムをドン！！”

```python
# ---
from datasets import load_dataset

ds = load_dataset("YxBxRyXJx/cut_vqav2_1022", split="train")
ds
```

D先生 ： “そもそも、これ（↑）は、何のデータセットなんですか？”

![imageVLM1-36-5](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-5.jpg)

QEU:FOUNDER ： “小生の自作データセットです。ただし、もともとは、このデータセット（↑）の抽出です。原版のデータセットのサイズが大きすぎるので読み込みに時間がかかります。自分が使用するためにコンパクトにまとめたんです。それでは、つづきにいきましょう。”

```python
# ---
from transformers import MllamaForConditionalGeneration, MllamaProcessor, BitsAndBytesCon-fig
from peft import LoraConfig, get_peft_model
import torch

# ---
torch.manual_seed(3)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load configuration for 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # Choose quantization type
    bnb_4bit_compute_dtype=torch.float16  # Set compute dtype
)

ckpt = "unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"

lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    lora_dropout=0.1,
    target_modules=['o_proj', 'k_proj', 'q_proj', 'v_proj'],
    #use_dora=True,  # Optional DoRA 
    init_lora_weights="gaussian"
)

# Load the model with quantization configuration
model = MllamaForConditionalGeneration.from_pretrained(
    ckpt,
    quantization_config=bnb_config,  # Use quantization config here
    torch_dtype=torch.float16,
    device_map="auto"
)

# ---
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

processor = MllamaProcessor.from_pretrained(ckpt)

```

QEU:FOUNDER ： “ここまでの結果は、こんな感じ（↓）です。”

![imageVLM1-36-6](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-6.jpg)

D先生 ： “PEFTとはいえ、書き換えるパラメタが0.05%しかないんですか？”

QEU:FOUNDER ： “この方法ですが、**当初はA100のGPUを使っているのにかかわらず、OUT OF MEMORYのエラーが常に出て来たのです**。ですから、メモリ量削減の対応として、コードを順次変更しています。では、さらにつづきを見てみましょう。”

```python
# ---
def process(examples):
    texts = [f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n<|image|>{example['question']} Answer briefly. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{example['multiple_choice_answer']}<|eot_id|>" for example in examples]
    images = [[example["image"].convert("RGB")] for example in examples]

    batch = processor(text=texts, images=images, return_tensors="pt", padding=True)
    labels = batch["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100 
    labels[labels == 128256] = -100  # Image token index
    batch["labels"] = labels
    batch = batch.to(torch.float16).to("cuda")

    return batch

# ---
from transformers import TrainingArguments
args = TrainingArguments(
    num_train_epochs=2,
    remove_unused_columns=False,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=2,
    warmup_steps=2,
    learning_rate=2e-5,
    weight_decay=1e-6,
    adam_beta2=0.999,
    optim="adamw_hf",
    save_total_limit=1,
    output_dir="drive/MyDrive/MURIYARI_lora",
    dataloader_pin_memory=False,
)

# ---
from transformers import Trainer
trainer = Trainer(
    model=model,
    train_dataset=ds,
    data_collator=process,
    args=args
)

# ---
trainer.train()

```

QEU:FOUNDER ： “ホラ・・・。ちゃんと学習がはじまっているでしょう？”

![imageVLM1-36-7](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-7.jpg)

D先生 ： “でも、途中で処理を中断していますね・・・（笑）。”

QEU:FOUNDER ： “これから2時間も学習処理がつづくんですよ。それも、高価なA100のGPUを使って・・・。このデータはVQA(Visual Q&A)だから、小生としては成果物もあまり興味もないし・・・。”

D先生 ： “せっかくのUnslothモデルを使って、A100を使って学習するの？”

QEU:FOUNDER ： “この方法のメリットとしては、Unslothモデルが軽いので学習後の推論で楽になるとは思います。個人的には、あまり推薦しないね。”

D先生 ： “懸案となっている、**Unslothのクラスの不具合を改善してくれないかなあ・・・**。”

![imageVLM1-36-8](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-8.jpg)

QEU:FOUNDER ： “彼らにも優先順位があるんです。かれらは随時結果を出してきているので、我々としては順番がくるのを待つしかないですね。”


## ～ まとめ ～

QEU:FOUNDER ： “なんか、えらい「祭り（↓）」が起きていますね。”

[![MOVIE1](http://img.youtube.com/vi/My_JJXwf_Ro/0.jpg)](http://www.youtube.com/watch?v=My_JJXwf_Ro "維新の音喜多駿さんに、社会の厳しさを教えてもらおう 能登や万博のことで御高説あるかも")

C部長 : “たしかに、面白おかしく言うと「ネット用語でいう祭り」ですが、この件は**政治のレベル低下の現れ**じゃないですか。それにしても、ひどいなあ・・・。“

![imageVLM1-36-9](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-9.jpg)

QEU:FOUNDER ： “この話（↓）も、同じ次元にある話なんでしょうね。ホント腐っていますね。上も、下も・・・。”

![imageVLM1-36-10](/2024-10-22-QEUR23_VIVLM36/imageVLM1-36-10.jpg)

C部長 : “**長い間の腐敗体制の結果**でしょ？そう簡単によくなるのかなあ・・・。 “

