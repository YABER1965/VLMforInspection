---
title: QEUR23_SMNW14 – Unslothの1shot-PROMPTでQWEN-VLMを学習してみる
date: 2024-11-30
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: VLMで外観検査をやってみる
---

## QEUR23_SMNW14 – Unslothの1shot-PROMPTでQWEN-VLMを学習してみる

## ～ QWEN-7Bは、残念ながら力不足か ～

D先生 ： “前回は、Llama-3.2-11b(VLM)で1SHOT-PROMPTを使った学習を行い、ZERO-SHOTで推論を行いました。それによると、出力のFORMATは良くなるが、推論の正確度はダメでした。今回は、最初にLlama3.2の再挑戦です。少しだけ改善したんでしょ？”

![imageSMNW3-5-1](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-1.jpg)

QEU:FOUNDER ： “1SHOT目のinstructionの文をかえました。具体的には、**ピンのアドレス表示用の英数字を明確にした**んです。”

You are a visual inspector who finds defects in products. Image 1 shows a group of pins (cylinders) on a product. This image shows the pins from a top view. There are alphanumeric characters above and left of the product image. Numbers 1 through 9 are placed above the product image. Also, three al-phabets U, C, and D are placed above the product image. These alphanumeric characters are used to generate an address where the defect is located, with the numbers indicating the location in X direc-tion and U, C, and D indicating the location in Y direction. Image 1 shows a red star placed on a pin as defect. You need to specify the location of this red star. After your visual inspection task, you need to describe type of image, quantity of pins, and defect(red star) position.

あなたは、製品の欠陥を見つける目視検査員です。画像1には、製品上のピン(シリンダー)のグループが示されています。この画像は、上から見たピンを示しています。製品画像の上と左に英数字があります。1から9の数字が製品画像の上に配置されています。また、U,C,Dという3つのアルファベットが製品画像の上に配置されています。これらの英数字は、数字がX方向の位置を、U,C,DはY方向の位置をしめし、それらが結合されて、欠陥がある場所のアドレスを生成するために使用されます。画像1には、欠陥として赤い星がピンの上に配置されています。この赤い星の位置を指定する必要があります。あなたは外観検査後、画像の種類、画像のピンの数、赤い星の位置を説明する必要があります。

C部長 ： “ここまではっきりと英数字の種類を明示すれば、VLMもわかってくれるでしょう。”

QEU:FOUNDER ： “たしかにLlama3.2-11b(VLM)は、instructionの意味をわかってくれました。ただし、残念ながら、肝心の「目があまり良くない」ようです。では、推論の結果をドン！！”

![imageSMNW3-5-2](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-2.jpg)

D先生 ： “まあ、Pre-trainには想定されていない正確に欠陥の位置を検出するタスクは難しいでしょうね。そうだ！！合格品を検出するケースはありませんでしたか？”

QEU:FOUNDER ： “ありますよ。見てみましょうか・・・。”

![imageSMNW3-5-3](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-3.jpg)

D先生 ： “やった、目標達成！ひょっとしてなんですが・・・。NSOARTCの処理を使わずに、**生データ(RAW)を使ったほうが、判別精度が良くなりませんか？**”

![imageSMNW3-5-4](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-4.jpg)

QEU:FOUNDER ： “実は、そんな気がします・・・（笑）。そもそも、**VLMの画像処理担当モデルは我々が開発したViTモデルよりもはるかに強力**なんです。わざわざ前処理しなくても識別してくれると思います。”

D先生 ： “じゃあ、次はQwenモデルを使ったトライアルをやってみましょうか。このモデルのパラメタ数は7b（70億）なので、llama-11b(110億)よりもモデルが軽量ですね。もちろん、Qwenモデルの学習データの中に、「今回と近いケース」があれば、判別がうまく行く希望があります。”

QEU:FOUNDER ： “じゃあ、ここではQwen用の推論用プログラムを晒します。Llamaモデルとの差異は、プロンプトの設計だけです。ドン！！”

```python
# ---
import torch
from unsloth import FastVisionModel # FastLanguageModel for LLMs

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit", # Llama 3.2 vision support
    "unsloth/Llama-3.2-11B-Vision-bnb-4bit",
    "unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit", # Can fit in a 80GB card!
    "unsloth/Llama-3.2-90B-Vision-bnb-4bit",

    "unsloth/Pixtral-12B-2409-bnb-4bit",              # Pixtral fits in 16GB!
    "unsloth/Pixtral-12B-Base-2409-bnb-4bit",         # Pixtral base model

    "unsloth/Qwen2-VL-2B-Instruct-bnb-4bit",          # Qwen2 VL support
    "unsloth/Qwen2-VL-7B-Instruct-bnb-4bit",
    "unsloth/Qwen2-VL-72B-Instruct-bnb-4bit",

    "unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit",      # Any Llava variant works!
    "unsloth/llava-1.5-7b-hf-bnb-4bit",
] # More models at https://huggingface.co/unsloth


# ----
# MODEL NAME
#model_name = "unsloth/Qwen2-VL-7B-Instruct-bnb-4bit"
model_name = "YxBxRyXJx/lora_Unsloth_qwen_1129"
model, tokenizer = FastVisionModel.from_pretrained(
    model_name,
    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for long context
)

```

D先生 ： “「model_name」には、今回学習したアダプタ名が記入されていますね。今回は無事にHugging Faceにアップされています。ファインチューニングで学習したときの様子はどうでしたか？”

![imageSMNW3-5-5](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-5.jpg)

QEU:FOUNDER ： “7bパラメタのモデルだけあって、**T4(GPU)でも十分学習できました**よ。”

![imageSMNW3-5-6](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-6.jpg)

D先生 ： “Qwenは軽量というのも、メリットと言えます。”

QEU:FOUNDER ： “いやあ・・・。ここまで軽量なのは、それなりの「意味（良し悪し）」があるんですよ。何はともあれ、推論用プログラムの晒しは続きます。”

```python
# ----
import numpy as np

# ----
# reading_pic
mx_image_DST = [["NA"]*2 for i in range(10)]
mx_image_RAW = [["NA"]*2 for i in range(10)]
# ---
# DST image - MATRIX
mx_image_DST[0] = ["combined_image_simple_no193.jpg", "X:1, Y:U"]
mx_image_DST[1] = ["combined_image_simple_no194.jpg", "X:3, Y:U"]
mx_image_DST[2] = ["combined_image_simple_no196.jpg", "X:5, Y:U"]
mx_image_DST[3] = ["combined_image_simple_no200.jpg", "X:7, Y:U"]
mx_image_DST[4] = ["combined_image_simple_no201.jpg", "X:3, Y:C"]
mx_image_DST[5] = ["combined_image_simple_no203.jpg", "X:5, Y:C"]
mx_image_DST[6] = ["combined_image_simple_no206.jpg", "X:7, Y:C"]
mx_image_DST[7] = ["combined_image_simple_no208.jpg", "X:3, Y:D"]
mx_image_DST[8] = ["combined_image_simple_no209.jpg", "X:5, Y:D"]
mx_image_DST[9] = ["combined_image_simple_no212.jpg", "X:7, Y:D"]
mx_image_DST = np.array(mx_image_DST)
#mx_image_DST

# ---
# RAW image - MATRIX
mx_image_RAW[0] = ["combined_image_simple_no192.jpg", "X:4, Y:U"]
mx_image_RAW[1] = ["combined_image_simple_no202.jpg", "X:7, Y:C"]
mx_image_RAW[2] = ["combined_image_simple_no204.jpg", "X:8, Y:D"]
mx_image_RAW[3] = ["combined_image_simple_no205.jpg", "X:1, Y:D"]
mx_image_RAW[4] = ["combined_image_simple_no207.jpg", "X:9, Y:U"]
mx_image_RAW[5] = ["combined_image_simple_no210.jpg", "X:5, Y:C"]
mx_image_RAW[6] = ["combined_image_simple_no211.jpg", "X:3, Y:D"]
mx_image_RAW[7] = ["combined_image_simple_no213.jpg", "X:1, Y:C"]
mx_image_RAW[8] = ["combined_image_simple_no215.jpg", "X:2, Y:C"]
mx_image_RAW[9] = ["combined_image_simple_no217.jpg", "X:3, Y:C"]
mx_image_RAW = np.array(mx_image_RAW)
mx_image_RAW

# ---
instruction = "You are visual inspector who judges whether a product is passed or defective. Both image1 and image2 show a group of pins (cylinders) on the product. These images are top view of the pins. The number and arrangement of pins in image1 and image2 are the same. First, you should confirm that number and arrangement of pins in image1 and image2 must be the same. Then, you should count the quantity of pins in image2. Image1 is an image of passed product. You should com-pare all of pins on the product in image1 and image2 one by one. If you found a significant difference on the pins, it is defined as defect. After visual inspection, you should describe image type, quantity of pins, your judgement, defect mode, and defect position."

#######################################
# ZERO SHOT INFERENCE
#######################################
# -----
# VLM Inference for 0shot
#import numpy as np
import random
from PIL import Image
from IPython.display import Markdown, display
import matplotlib.pyplot as plt

# -----
# 作画関数(for 2 pics)
def draw_two_pic(arr_image, arr_title):
    # グラフ化
    fig = plt.figure(figsize=(12,8))
    fig.tight_layout(rect=[0,0,0.4,0.4])
    # ---
    plt.subplot(1,2,1)
    plt.title(arr_title[0])
    plt.imshow(arr_image[0])
    # ---
    plt.subplot(1,2,2)
    plt.title(arr_title[1])
    plt.imshow(arr_image[1])
    # 画像の表示
    plt.show()

# ---
def generate_index():
    # ---
    # index for Image1
    list_range = list(range(10))
    j_img1 = random.choice(list_range)
    # ---
    # index for Image1
    j_img2 = random.choice(list_range)

    return j_img1, j_img2

# ---
def read_imagefile(mx_image_DST, mx_image_RAW, dataset_temp, icount):
    # ---
    # ランダムイメージを選択する
    str_type = dataset_temp[icount]["type"]
    j_img1, j_img2 = generate_index()
    # ---   
    if str_type == 'CYLN_DST':
        str_dir1 = "drive/MyDrive/passed_image/DST/"
        filename1 = mx_image_DST[j_img2, 0]
        image1 = Image.open(str_dir1 + filename1)         
    else:
        str_dir1 = "drive/MyDrive/passed_image/RAW/"
        filename1 = mx_image_RAW[j_img2, 0]
        image1 = Image.open(str_dir1 + filename1).convert("RGB")
    # ---
    image2 = dataset_temp[icount]["image"]
    # ---
    str_title1 = f"image1: {filename1}"
    str_title2 = f"Image2 - DATA NO:{icount}"
    # ---
    arr_image = [image1, image2]
    arr_title = [str_title1, str_title2]

    return arr_image, arr_title

# ---
# データ番号(icount)を指定すること
icount = 0
arr_image, arr_title = read_imagefile(mx_image_DST, mx_image_RAW, dataset_test, icount)
# 画像を表示する
draw_two_pic(arr_image, arr_title)

# ---
# VLM Inference for 0shot
def convert_ZERO_message(sample):
    # ----
    str_dir = sample["dir"]
    str_position = sample["position"]
    str_type = sample["type"]
    str_bbox = sample["bbox_str"]
    # ----
    messages = [
        { "role": "user",
          "content" : [
            {"type" : "text",  "text"  : instruction},
            {"type" : "image", "image" : "image1"},
            {"type" : "image", "image" : "image2"} ]
        }
    ]
    # ----
    index_bbox = str_bbox.index("<loc")
    str_defect = str_bbox[:index_bbox]
    str_defective = "passed"
    if str_defect != "passed":
        str_defective = "defective"
    # ----
    str_answer = f"- image type->{str_type}\n- quantity of pins->27 pins\n- your judgement->{str_defective}\n- defect mode->{str_defect}\n- defect position->{str_position}"

    return messages, instruction, str_answer

# ---
# INSTRUCTION を消してしまう
def eliminate_prompt(response):
    # ---
    split_string = '<|im_start|>assistant'
    index_response = response[0].index(split_string)
    cut_response = response[0][index_response+len(split_string):]
    out_response = cut_response.replace("<|im_end|>","")
    #print(cut_response)
    
    return out_response

# ---
# メッセージを生成する
sample = dataset_test[icount]
message, instruction, str_answer = convert_ZERO_message(sample)
print("--- message ---")
print(message)
print("--- instruction ---")
print(instruction)
print("--- str_answer ---")
print(str_answer)

# ---
FastVisionModel.for_inference(model) # Enable for inference!

# ---
# 3つのイメージを使う
images = arr_image
input_text = tokenizer.apply_chat_template(message, add_generation_prompt = True)
inputs = tokenizer(
    images,
    input_text,
    add_special_tokens = False,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)
response = tokenizer.batch_decode(outputs)
# ---
print("--- INSTRUCTION ---")
display(Markdown(f"<b>{instruction}</b>"))
print("--- AI RESPONSE ---")
display(Markdown(f"<b>{eliminate_prompt(response)}</b>"))
print("--- DATASET ANSWER ---")
display(Markdown(f"<b>{str_answer}</b>"))

```

D先生 ： “やっと推論結果が出てきました。あれあれ・・・？”

![imageSMNW3-5-7](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-7.jpg)

C部長： “あれ？「defect position」の出力結果が、ぜんぜんダメですね。せっかく1shot-PROMPTで学習したはずなのに・・・。”

QEU:FOUNDER ： “もう一枚だけ、別の推論した結果を見てみましょうか・・・。”

![imageSMNW3-5-8](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-8.jpg)

C部長 ： “あ～あ・・・。Qwenは、我々がせっかく準備した1shot-promptを使いこなせないモデルのようです。まあ、LVMモデルが悪いのか、UnslothのインスタンスがQwenモデルの1shotに対応していないかもしれないですね。”

QEU:FOUNDER ： “残るモデルは、PixtralとLlavaですね。Pixtral は12bなので、Pixtralだけをやってみようかな。Pixtralが1shot学習を対応しているモデルであることを祈りましょう。”


## ～ まとめ ～

QEU:FOUNDER ： “これ（↓）を見たとき、「あ～あ・・・」と思いましたよね。”

![imageSMNW3-5-9](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-9.jpg)

C部長 : “B爺さんは後列か・・・。時代の流れを実感しますよね。この会議のテーマは**「ECONOMIC」**でしょ？もう、A国は経済で世界的な存在感を発揮できないんですね。”

[![MOVIE1](http://img.youtube.com/vi/jN4Ifaz92lw/0.jpg)](http://www.youtube.com/watch?v=jN4Ifaz92lw "中美國運分水嶺！消失的拜登，美國一霸時代結束淪為配角！｜美國盟友覺醒，英國開始對華尋求經濟，摩洛哥成為中國進駐歐洲橋頭堡！")

QEU:FOUNDER ： “J国の存在感も落ちている。もちろん、亡くなった藤森さんの件はわかる。それでも、こういう写真を撮られるということは、そもそも**「国力の可視化なのだ」**と・・・。RIUNIONE DEI MINISTRI DEGLI ESTERI（外務大臣会合）においても、以下同文・・・。”

![imageSMNW3-5-10](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-10.jpg)


C部長 : “J国、かろうじて踏みとどまる・・・（笑）。”

QEU:FOUNDER ： “そうか？どんなに無理な「コミット(commit)」をしたんだろうかと、心配になってくるのだが・・・。しゃあない。**近所でもめ事**が起きなければ、良しとするしかないよね。”

![imageSMNW3-5-11](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-11.jpg)

C部長 : “ここしばらくは、Bさんのあがきで緊張があるのかな？半年したら、Tさんが出てくるから、もうちょっと落ち着いてくれることを願いたい。”

QEU:FOUNDER ： “もうね・・・。ケンカの前に、**「体格の圧倒的な差」**を理解しなきゃ・・・。あがいても無理なんですよ。”

![imageSMNW3-5-12](/2024-11-30-QEUR23_SMNW14/imageSMNW3-5-12.jpg)

C部長 : “A国の戦略は、「Offshore balancer」ですか・・・？U-R紛争では、A国はU国に大量のカネを与えたが、J国には与えんでしょう。”

### 孔子様曰く：「恒産なくして恒心無し。恒心無くしてワンチャンあり。」

QEU:FOUNDER ： “この手の「ワンチャン」は、特に勘弁してもらいたい。”

