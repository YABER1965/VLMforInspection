---
title: QEUR23_VIVLM32 : Florence2ã§finetuneã‚’ODç”¨ã®finetuneã‚’ã—ã¦ã¿ã‚‹
date: 2024-10-14
tags: ["QEUã‚·ã‚¹ãƒ†ãƒ ", "ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹", "Pythonè¨€èª", "Vision Transformer", "LLM", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", "Fine-tuning", "Vision language Model"]
excerpt: Vision Transformer(ViT)ã‚’ã‚„ã£ã¦ã¿ã‚‹
---

## QEUR23_VIVLM32 : Florence2ã§finetuneã‚’ODç”¨ã®finetuneã‚’ã—ã¦ã¿ã‚‹

## ï½ ã‚„ã£ã±ã‚Šã€Florence2ã®æ–¹ãŒã„ã„ã‹ã‚‚ ï½

QEU:FOUNDER ï¼š â€œã“ã“ã§æ–¹é‡ã‚’å¤‰æ›´ã—ã¾ã™ï¼ä»Šå›ã¯ã€Florenceï¼ˆä»¥ä¸‹FLï¼‰ã‚’ä½¿ã£ãŸOD(Object Detection:ç‰©ä½“æ¤œå‡ºï¼‰ã‚’ã‚„ã‚Šã¾ã—ã‚‡ã†ã€‚â€

![imageVLM1-32-1](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-1.jpg)

Då…ˆç”Ÿ ï¼š â€œã‚ã‚Œï¼Ÿ TUå›½ã®æˆæœç‰©ï¼ˆâ†“ï¼‰ã‚’ä½¿ã†ã‚“ã˜ã‚ƒãªã‹ã£ãŸã®ã§ã™ã‹ï¼Ÿâ€

![imageVLM1-32-2](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-2.jpg)

QEU:FOUNDER ï¼š â€œMODEL CARDã«finetuneç”¨ã®notebookãŒã‚ã£ãŸã‚“ã§ã™ã€‚ã§ã‚‚ã€ãªã‚“ã§ã‹ãªã‚ãƒ»ãƒ»ãƒ»ã€ã†ã¾ãå‹•ã‹ãªã„ã§ã™ã€‚ãƒ»ãƒ»ãƒ»ã¨ã„ã†ã‹ã€ã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã¦ã‚‚ã„ãã¤ã‹çŸ›ç›¾ãŒã‚ã‚Šã€ãã‚‚ãã‚‚ã‚¢ãƒ¬ã¯å‹•ãã®ã‹ãªã‚¡ãƒ»ãƒ»ãƒ»ã€‚Florence-2ã§èª¿æŸ»ã™ã‚‹ã¨ã€**ã€Œã¡ã‚ƒã‚“ã¨å‹•ããŸãŸãå°notebookãŒã‚ã£ãŸã€**ã®ã§ã€ãã‚Œã‚’æ”¹é€ ã—ã¦ä½¿ã„ã¾ã™ã€‚â€

![imageVLM1-32-3](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-3.jpg)

QEU:FOUNDER ï¼š â€œã•ã¦ã€ä¾‹ã®TraVisionLMã¨åŒä¸€ã®ä½œè€…ãªã®ã ãŒã€å½¼ã¯ãªã‹ãªã‹è‰¯ã„VLMç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¦ã„ã‚‹ã‚“ã§ã™ã€‚ã‚‚ã¡ã‚ã‚“ã€ã‚³ãƒ¡ãƒ³ãƒˆã¯TUèªã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã ãŒãƒ»ãƒ»ãƒ»ã€‚â€

![imageVLM1-32-4](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-4.jpg)

Då…ˆç”Ÿ ï¼š â€œã§ã‚‚ã€FOUNDERã¯ã€ãã®å„ªç§€ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚ã–ã‚ã–æ›¸ãæ›ãˆãŸã‚“ã§ã™ã‚ˆã­ã€‚ãªãœã§ã™ã‹ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œãã®COCOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä½•ã—ã‚å¤§ãã™ãã¾ã™ã€‚å°ç”Ÿã®ä½¿ç”¨ç›®çš„ã¯å¤§ã’ã•ãªã‚‚ã®ã§ã¯ãªã„ã§ã™ã€‚ã‚ˆã‚Šä½ã‚³ã‚¹ãƒˆã§finetuningãŒã§ãã‚‹ã‚ˆã†ã«å¤§å¹…ã«å‰Šã‚Šã¾ã—ãŸã€‚ã‚ã¨ã¯ã€bbox_strã®ã‚³ãƒ¡ãƒ³ãƒˆã®æ§‹é€ ãŒå¤‰ã‚ã£ã¦ã„ã¾ã™ã€‚ã•ã¦ã€finetuningã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’é †ç•ªã«è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚â€

```python
# ---
from datasets import load_dataset

dataset = load_dataset("YxBxRyXJx/cut_TRV_train_10116")
#dataset["train"][0]

# ---
import torch

# ---
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ ã€Œbbox_strã€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å«ã¾ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ã“ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«ã€Florence-2 ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚
# Let's create train and validation splits.
split_dataset = dataset["train"].train_test_split(test_size=0.2, shuffle=True)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]
print("Len train dataset: ", len(train_dataset))
print("Len eval dataset: ", len(eval_dataset))
# ---
test_dataset = dataset["test"]
print("Len test dataset: ", len(test_dataset))

```

QEU:FOUNDER ï¼š â€œã“ã“ã¾ã§ã§ã€æˆ‘ã€…ã®ä½œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç„¡äº‹ã«èª­ã¿è¾¼ã¾ã›ã‚‹ã“ã¨ãŒå‡ºæ¥ã¾ã—ãŸã€‚â€

![imageVLM1-32-5](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-5.jpg)

Då…ˆç”Ÿ ï¼š â€œãªã‚“ã‹ã€ãƒ‡ãƒ¼ã‚¿æ•°ã¯ç«¯æ•°ã«ãªã£ã¦ã„ã¾ã™ã‚ˆã­ã€‚ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚’ï¼‰å‰Šã‚Šã¾ã—ãŸã‹ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œ**ç™½é»’ç”»åƒã‚’å‰Šã‚Šã¾ã—ãŸ**ã€‚Finetuneã§é€”ä¸­ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã®ã§ã€åŸå› ã‚’èª¿ã¹ã¦ã¿ãŸã‚‰ç™½é»’ç”»åƒã§ã—ãŸã€‚ã˜ã‚ƒã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ã‚‡ã†ã€‚â€

```python
# ---
# Load Model and Processor
from transformers import AutoProcessor, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("microsoft/Florence-2-large-ft", revi-sion="refs/pr/10", trust_remote_code=True, device_map="cuda") # load the model on GPU
processor = AutoProcessor.from_pretrained("microsoft/Florence-2-large-ft", revision="refs/pr/10", trust_remote_code=True)

```

Då…ˆç”Ÿ ï¼š â€œã“ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ã€ç”»åƒè¡¨ç¤ºã‚’ã—ã¦ã¿ãŸã‚“ã§ã™ã­ã€‚â€

```python
# ---
# Let's define a plotting function to show the detected objects in a given image with an example from the training dataset.
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# ---
def run_example(task_prompt, image, max_new_tokens=128):
    prompt = task_prompt
    inputs = processor(text=prompt, images=image, return_tensors="pt")
    generated_ids = model.generate(
      input_ids=inputs["input_ids"].cuda(),
      pixel_values=inputs["pixel_values"].cuda(),
      max_new_tokens=max_new_tokens,
      early_stopping=False,
      do_sample=False,
      num_beams=3,
    )
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    parsed_answer = processor.post_process_generation(
        generated_text,
        task=task_prompt,
        image_size=(image.width, image.height)
    )
    return parsed_answer

# ---
def plot_bbox(image, data):
   # Create a figure and axes
    fig, ax = plt.subplots()

    # Display the image
    ax.imshow(image)

    # Plot each bounding box
    for bbox, label in zip(data['bboxes'], data['labels']):
        # Unpack the bounding box coordinates
        x1, y1, x2, y2 = bbox
        # Create a Rectangle patch
        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')
        # Add the rectangle to the Axes
        ax.add_patch(rect)
        # Annotate the label
        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))

    # Remove the axis ticks and labels
    ax.axis('off')

    # Show the plot
    plt.show()

# ---
example_id = 250
image = train_dataset[example_id]["image"]
#image

# notice here that <OD> task prompt is used. This task prompt is already used in training the Flor-ence-2 model checkpoints for object detection.
parsed_answer = run_example(task_prompt="<OD>", image=image)
plot_bbox(image, parsed_answer["<OD>"])

```

QEU:FOUNDER ï¼š â€œPre-trainã®çŠ¶æ…‹ã®ãƒ¢ãƒ‡ãƒ«ã®å‡ºæ¥æ „ãˆã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚â€

![imageVLM1-32-6](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-6.jpg)

Då…ˆç”Ÿ ï¼š â€œã‚ã‚Œï¼ŸFlorence-2ã¸ä¾›çµ¦ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã©ã“ã«ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œTraVisionLMãƒ¢ãƒ‡ãƒ«ã¨ã¯é•ã„ã€**Florence-2ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€Œ<OD>ã€ã§ã™**ã€‚ã“ã‚Œã¯ã€ã€ŒObject Detectionã‚’ã—ã¦ãã‚Œã€ã¨ã„ã†æ„å‘³ã§ã™ã€‚ãã‚Œã§ã¯ã€ã‚³ãƒ¼ãƒ‰ã‚’ã¤ã¥ã‘ã¾ã™ã€‚â€

```python
# ----
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã® OOM ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã«ã€ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ« ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ãƒ•ãƒªãƒ¼ã‚ºã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã—ã¦ã„ã¾ã™ã€‚Florence-2 ã«ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºæ©Ÿèƒ½ãŒã™ã§ã«å‚™ã‚ã£ã¦ã„ã‚‹ãŸã‚ã€å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã®ã§ã¯ãªãã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼/ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã‚ˆã‚Šå¼·åŠ›ãªãƒªã‚½ãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆã‚„ã€ãƒ¢ãƒ‡ãƒ«ã«ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã•ã›ãŸã„å ´åˆã¯ã€ã“ã®è¨­å®šã‚’è‡ªç”±ã«æ¤œè¨ã—ã¦ãã ã•ã„ã€‚
for param in model.vision_tower.parameters():
  param.requires_grad = False

model_total_params = sum(p.numel() for p in model.parameters())
model_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Number of trainable parameters {model_train_params} out of {model_total_params}, rate: {model_train_params/model_total_params:0.3f}")

```

QEU:FOUNDER ï¼š â€œã“ã‚Œï¼ˆâ†“ï¼‰ã‚’ã¿ã‚‹ã¨ã€ä»Šå›ã®è¨“ç·´å¯¾è±¡ã®ãƒ‘ãƒ©ãƒ¡ã‚¿æ•°ãŒã‚ã‹ã‚Šã¾ã™ã€‚â€

![imageVLM1-32-7](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-7.jpg)

Då…ˆç”Ÿ ï¼š â€œãˆã£ï¼ï¼Ÿãƒ¢ãƒ‡ãƒ«ã®åŠåˆ†ä»¥ä¸Šã‚‚å¤‰ãˆã‚‹ã‚“ã§ã™ã‹ï¼ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œå°ç”Ÿã‚‚ã€ã“ã“ã¾ã§ãã‚‹ã¨ï¼ˆè¨ˆç®—æ™‚é–“ãŒï¼‰ç„¡é§„ã ã¨æ€ã†ã‚“ã ã‘ã©ãªã‚ãƒ»ãƒ»ãƒ»ã€‚10ï¼…ä»¥ä¸‹ã«æŠ‘ãˆã¦ã»ã—ã„ã¨ã“ã‚ã§ã™ã€‚æ¬¡ã«è¡Œãã¾ã—ã‚‡ã†ã€‚ã„ã‚ˆã„ã‚ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã™ã€‚â€

```python
# ---
IGNORE_ID = -100 # Pytorch ignore index when computing loss
MAX_LENGTH = 512

def collate_fn(examples):
    task_prompt = "<OD>"

    prompt_texts = [task_prompt for _ in examples]
    label_texts = [example["bbox_str"] for example in examples]
    images = [example["image"] for example in examples]

    inputs = processor(
        images=images,
        text=prompt_texts,
        return_tensors="pt",
        padding="longest",
        max_length=MAX_LENGTH,
    )

    labels = processor.tokenizer(
        label_texts,
        return_tensors="pt",
        padding="longest",
        max_length=MAX_LENGTH,
        return_token_type_ids=False, # no need to set this to True since BART does not use token type ids
    )["input_ids"]

    labels[labels == processor.tokenizer.pad_token_id] = IGNORE_ID # do not learn to predict pad to-kens during training

    return_data = {**inputs, "labels": labels}
    return return_data

# Test the data collator.
collated_examples = collate_fn([train_dataset[0], train_dataset[6]])
#collated_examples

# ---
# ä»¥ä¸‹ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªç”±ã«èª¿æ•´ã—ã¦ãã ã•ã„ã€‚Florence-2 ã®ã‚ˆã†ãªå°ã•ãªãƒ¢ãƒ‡ãƒ«ã¯ã€å¾®èª¿æ•´ä¸­ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿ã«ç°¡å˜ã«éå‰°é©åˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ¯”è¼ƒçš„ä½ã„å­¦ç¿’ç‡ãŒä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
# ---
from transformers import TrainingArguments

args=TrainingArguments(
    output_dir="drive/MyDrive/FTmodels/Florence-2-OD-COCO",
    num_train_epochs=2,
    learning_rate=3e-6,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy="epoch",
    logging_strategy="epoch",
    eval_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=False, # we will manually push model to the hub at the end of training
    label_names=["labels"],
    remove_unused_columns=False,  # needed for data collator
)
# ---
from transformers import Trainer
trainer = Trainer(
    model=model,
    tokenizer=processor,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=collate_fn, # dont forget to add custom data collator
    args=args
)
# ---
trainer.train()

```

Då…ˆç”Ÿ ï¼š â€œå­¦ç¿’ã¯ã€ãŸã£ãŸï¼’ã‚¨ãƒãƒƒã‚¯ãªã‚“ã§ã™ã‹ï¼Ÿâ€

![imageVLM1-32-8](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-8.jpg)

QEU:FOUNDER ï¼š â€œæ™‚é–“ãŒã‹ã‹ã‚‹ã®ã­ã€ã‚³ãƒ¬ãƒ»ãƒ»ãƒ»ã€‚ã‚‚ã¨ã‚‚ã¨ãƒ†ã‚¹ãƒˆç›®çš„ãªã®ã§2å›ã§ååˆ†ã§ã™ã€‚ã˜ã‚ƒã‚ã€æœ€å¾Œã«ãƒ¢ãƒ‡ãƒ«æ¨è«–ã®å‡ºæ¥æ „ãˆã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚â€

```python
# ---
# æ¤œè¨¼ã‚»ãƒƒãƒˆã®ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã€å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ãƒ—ãƒ­ãƒƒãƒˆé–¢æ•°ã¨ç”Ÿæˆé–¢æ•°ã¯ã™ã§ã«å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚
example_id = 100
image = test_dataset[example_id]["image"]
parsed_answer = run_example("<OD>", image=image)
plot_bbox(image, parsed_answer["<OD>"])

```

Då…ˆç”Ÿ ï¼š â€œã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ã€Œinsanã€ã£ã¦ã€ä½•ã§ã™ã‹ï¼Ÿâ€

![imageVLM1-32-9](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-9.jpg)

QEU:FOUNDER ï¼š â€œTUèªã§ã€Œäººã€ã¨ã„ã†æ„å‘³ã§ã™ã€‚ã¨ã‚Šã‚ãˆãšã€ä»Šå›ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã§finetuning ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒå‹•ãã“ã¨ãŒæ¤œè¨¼ã•ã‚Œã¾ã—ãŸã€‚ã„ã‚ˆã„ã‚ˆã€å¤–è¦³æ¤œæŸ»è‡ªå‹•æ©Ÿç”¨ã®NSOARTCç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã‚Šã¾ã—ã‚‡ã†ã€‚â€

![imageVLM1-32-10](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-10.jpg)

Då…ˆç”Ÿ ï¼š â€œä¾‹ã«ã‚ˆã£ã¦ã€ã€Œã‚ã‚Šã‚ã‚ã›ã®ãƒ‡ãƒ¼ã‚¿ã€ã§ã‚„ã‚‹ã‚“ã§ã—ã‚‡ï¼Ÿâ€

QEU:FOUNDER ï¼š â€œå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã£ã¦ã€1000ä»¶ã‚ã‚Œã°ã„ã„æ–¹ã˜ã‚ƒãªã„ã‹ãªï¼Ÿâ€

Då…ˆç”Ÿ ï¼š â€œLoRAãŒä½¿ãˆã‚Œã°ã‚ˆã‹ã£ãŸã®ã«ã­ãƒ»ãƒ»ãƒ»ã€‚â€


## ï½ ã¾ã¨ã‚ ï½

### ãƒ»ãƒ»ãƒ» å‰å›ã®ã¤ã¥ãã§ã™ ãƒ»ãƒ»ãƒ»

QEU:FOUNDER ï¼š â€œã•ã£ãã€é¸æŒ™ã®ç«‹å€™è£œäºˆå®šè€…ä¸€è¦§ã‚’é•·ã„é–“è¦‹ã¦ãŸã‚“ã ã‘ã©ã€ã‚‚ã†é¢ç™½ãã£ã¦ã—ã‹ãŸãŒãªã„ã€‚å°ç”Ÿã®ç¬¬ä¸€å°è±¡ã‚‚ã“ã®äººï¼ˆâ†“ï¼‰ã¨åŒã˜ã§ã™ã‚ˆã€‚ã¡ã‚‡ã£ã¨ã€å‡ºé¦¬ã®ä¹±é›‘ã•ãŒã²ã©ã™ããƒ»ãƒ»ãƒ»ã€‚ä»Šå›ã¯ã€ä¸‹éƒ¨æ§‹é€ ã®æœ¬è³ªçš„ãªå¤‰åŒ–ãŒå¼•ãèµ·ã“ã™ã€**å›½å®¶æ§‹é€ ã®æ­´å²çš„ãªå¤§å¤‰åŒ–ã®ä¸€æ­©æ‰‹å‰**ãªã‚“ã˜ã‚ƒãªã„ã§ã—ã‚‡ã†ã‹ãƒ»ãƒ»ãƒ»ã€‚â€

![imageVLM1-32-11](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-11.jpg)

QEU:FOUNDER ï¼š â€œé¢ç™½ã„ã‚“ã ã‚ˆã­ã€‚Aå›½ã§èµ·ã“ã£ã¦ã„ã‚‹ã“ã¨ã¯ã€Jå›½ã¨ã»ã¨ã‚“ã©åŒã˜ãªã‚“ã ã‚ˆã­ã€‚æ”¿æ²»å®¶ã«ãªã‚ŠãŸãŒã‚‹ã‚¨ãƒªãƒ¼ãƒˆã¯ã€(è‡ªåˆ†ã®)ãŠé‡‘å„²ã‘ã®ã“ã¨ã—ã‹è€ƒãˆã¦ãŠã‚‰ãšã€ç†å¿µã‚’æŒã£ã¦ã„ãªã„ã€‚â€

[![MOVIE1](http://img.youtube.com/vi/TI1ivRzF1SA/0.jpg)](http://www.youtube.com/watch?v=TI1ivRzF1SA "ææ…Œãƒ»ãƒ‘ãƒ³ãƒ‡ãƒŸãƒƒã‚¯ãƒ»æˆ¦äº‰â€¦21ä¸–ç´€èª­ã¿è§£ãã¨ä¸–ç•Œã®è¡Œæ–¹")

Céƒ¨é•· : â€œAå›½ã®2å¤§æ”¿å…šåˆ¶ã¯ã€æ˜”ã¯ãƒªãƒ™ãƒ©ãƒ«ã¨ä¿å®ˆã¨è¨€ã‚ã‚Œã¾ã—ãŸã€‚ã„ã¾ã¯ã€ã»ã¨ã‚“ã©åŒã˜ã§ã™ã€‚ã‚€ã—ã‚ã€ã„ã‚ã‚†ã‚‹ãƒªãƒ™ãƒ©ãƒ«å´ãŒç‹‚æš´åŒ–ã—ã¦ã„ã¾ã™ã€‚â€œ

QEU:FOUNDER ï¼š â€œç‹‚æš´åŒ–ï¼Ÿç‰©é¨’ãªè¨€è‘‰ã‚’ãƒ»ãƒ»ãƒ»ã€‚â€

![imageVLM1-32-12](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-12.jpg)

Céƒ¨é•· : â€œãã†ã­ãˆã€ç‹‚æš´ã¨ã„ã†è¡¨ç¾ãŒå¦¥å½“ã ã­ãˆãƒ»ãƒ»ãƒ»ã€‚ãªã‚“ã¨ã‹ãªã‚‰ãªã„ã‚‚ã®ã‹ãƒ»ãƒ»ãƒ»ã€‚ â€œ

![imageVLM1-32-13](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-13.jpg)

QEU:FOUNDER ï¼š â€œã“ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒ³ï¼ˆâ†‘ï¼‰ã«ã€ä¸­æ±å•é¡Œã®è§£æ±ºã‚’ã‚„ã£ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ï¼â€

Céƒ¨é•· : â€œã¯ï¼Ÿãªãƒ»ãƒ»ãƒ»ãªã«ã‚’ã‚„ã£ã¦ã‚‚ã‚‰ã†ï¼ï¼Ÿãã‚Œã«ã—ã¦ã‚‚ã€**çµŒæ¸ˆå•é¡Œã‚’å‰æã«ã—ã¦å°Šå³ã€‡ã‚’è­°è«–ã™ã‚‹**ã®ï¼Ÿã²ã©ã„ãªã‚ãƒ»ãƒ»ãƒ»ã€‚ â€œ

![imageVLM1-32-14](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-14.jpg)

Céƒ¨é•· : â€œã¸ãˆãƒ»ãƒ»ãƒ»ã€‚**ã“ã†ã„ã†è¡Œç‚ºã‚’ã€Œå§¥æ¨å±±ã€ã¨ã„ã†**ã®ã‹ãƒ»ãƒ»ãƒ»ã€‚ãŠè‹¥ã„ã®ã«ç‰©çŸ¥ã‚Šãªäººã­ğŸ’›ã¨ã¦ã‚‚å‹‰å¼·ã«ãªã£ãŸã‚ãƒ»ãƒ»ãƒ»ã€‚ãªã‚“ã ã€**ã€Œè¤‡æ•°ã®äººãŸã¡ã€ãŒã“ã‚“ãªã‚¹ã‚´ã‚¤ã“ã¨ã‚’ã€ã¾ã˜ã‚ã«å‰å‘ãã«è­°è«–ã—ã¦ã„ã‚‹**ã‚ã‘ã§ã™ã­ã€‚ â€œ

![imageVLM1-32-15](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-15.jpg)

QEU:FOUNDER ï¼š â€œãªã‚“ã‹ã€æ”¿å…šå…¨ä½“ãŒãã†ã„ã†æ„è¦‹ã®ã‚ˆã†ã§ã™ã­ã€‚ã€Œé«˜é½¢è€…ã«ã¯å°Šå³ã€‡ã€ãªã‚“ã§ã—ã‚‡ï¼Ÿãã†é ããªã„è·é›¢ã«ã€Œèº«ä½“éšœç¢è€…ã«ã¯å°Šå³ã€‡ã€ãŒã‚ã‚Šã¾ã™ã€‚ã‚‚ã†ã¡ã‚‡ã£ã¨é›¢ã‚ŒãŸä½ç½®ã«ã€ã€ŒADHDã«ã¯å°Šå³ã€‡ã€ã¨ã„ã†è­°è«–ãŒã‚ã‚Šã¾ã™ã€‚â€

Céƒ¨é•· : â€œãšã‚“ãšã‚“è­°è«–ã™ã‚‹ã¨ã€**ã€Œã‚ªãƒ¬æ§˜ã®æ°—ã«å…¥ã‚‰ãªã„å¥´ã«ã¯å°Šå³ã€‡ã€ã¨ã„ã†çµè«–ã«ç€åœ°ã™ã‚‹**ã§ã—ã‚‡ã†ã€‚ã‚ã‚ã€**çµŒæ¸ˆãŒå‰æã®å°Šå³ã€‡è­°è«–**ã¨ã„ã†ã®ã¯ã€ã„ã‚„ã ãªã‚ãƒ»ãƒ»ãƒ»ã€‚ ã ã‘ã©ã€ãªãœã“ã®äººãŒä¸­æ±å•é¡Œã‚’è§£æ±ºã™ã‚‹ã®ï¼Ÿâ€œ

![imageVLM1-32-16](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-16.jpg)

QEU:FOUNDER ï¼š â€œå›½æ°‘æ°‘ã€‡å…šã®ãŠã˜ã•ã‚“ã€ã¨ã£ã¦ã‚‚é¦–ç›¸ã«ãªã‚ŠãŸã„ã‚“ã§ã—ã‚‡ï¼Ÿã˜ã‚ƒã‚ã€ã‚ã®å›½ã«ã„ã£ã¦é¦–ç›¸ã«ãªã£ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ã€‚ã‚ãã“ã®äººãŸã¡ã€ã¿ã‚“ãªã‚·ãƒ§ãƒƒã‚¯ã‚’å—ã‘ã¾ã™ã‚ˆã€‚**ã€Œè‡ªå›½å†…ã§è‡ªåˆ†ã‚’ã‚¸ã‚§ãƒã‚µã‚¤ãƒ‰ã™ã‚‹å¥´ãŒç¾ã‚ŒãŸã€**ã£ã¦ãƒ»ãƒ»ã€‚â€

Céƒ¨é•· : â€œ**ã€ŒWe are pushing self-genxcxde in our country!!ã€**ãƒ»ãƒ»ãƒ»ã€‚ã•ã™ãŒã«ã€ã‚ã®å›½ã®ä¸­ã®äººãŸã¡ã‚‚ã€è‡ªåˆ†ã®ï½±ï¾ã•ã«ç›®ãŒè¦šã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚â€œ

![imageVLM1-32-17](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-17.jpg)

QEU:FOUNDER ï¼š â€œä»Šå›ã®é¸æŒ™ã¯ã‚¨ã‚­ã‚µã‚¤ãƒ†ã‚£ãƒ³ã‚°ã ãªã‚ãƒ»ãƒ»ãƒ»ã€‚ä¸å…šã‚ˆã‚Šã‚‚ã€ã‚€ã—ã‚é‡å…šã®æ–¹ãŒç›¸å½“å¤‰ã‚ã‚‹ã¨æ€ã†ã‚ãƒ»ãƒ»ãƒ»ã€‚â€

