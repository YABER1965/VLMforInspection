---
title: QEUR23_VIVLM37 – GOOGLE-VITモデルでアテンションマップを描いてみる
date: 2024-10-24
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "Vision language Model"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VIVLM37 – GOOGLE-VITモデルでアテンションマップを描いてみる

## ～ やはり、「周り」はどんどん良くしてくれます ～

QEU:FOUNDER ： “久々に、**アテンション・マップ**を描いてみましょう。”

![imageVLM1-37-1](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-1.jpg)

D先生 ： “えっ！また、いきなり昔にやったことをやるんですか？”

QEU:FOUNDER ： “さっき、昔に制作したアテンション・マップのコードを久々に動かそうと思ったんです。しかし、うまく動きませんでした。そこで、思わずこの人（↓）を思い出しました。”

![imageVLM1-37-2](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-2.jpg)

QEU:FOUNDER ： “この人って、この業界では、ひそかに有名なブロガーなのだが、本業はエンジニアなんですよね。”

D先生 ： “この人は、なんでも試してブログにしますよね。おそろしく勤勉な人だと思います。”

![imageVLM1-37-3](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-3.jpg)

QEU:FOUNDER ： “自分のテリトリ界隈の技術がドンドン進歩するんで、自分が何かのシステムを開発するにしても、周りの状況をきちんとモニターしておかないといけないんです。自分だけでは、とうてい完結しない改善サイクル、**「DMAICSサイクル」**にはまった人です。・・・というわけで、周りが進歩したんだから、ちょっとレビューをしてみましょう。いきなりですが、プログラムをドン！！”

```python
##############################
# 普通のViT推論
##############################
# ---
from transformers import ViTFeatureExtractor, ViTForImageClassification
from PIL import Image
import requests
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
image

```

QEU:FOUNDER ： “この画像の猫ちゃんから、アテンション・マップをつくりたい。”

![imageVLM1-37-4](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-4.jpg)

D先生 ： “ふむふむ・・・。今回は、アップデートしたので、昔よりも、よりよいモデルを使えるわけですね。なにしろ、フリーで使えるのがありがたい。”

```python
# ---
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-large-patch32-384')
model = ViTForImageClassification.from_pretrained('google/vit-large-patch32-384')
inputs = feature_extractor(images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])

```

QEU:FOUNDER ： “まずは、「猫さん」と分類されましたね。えっ！あれあれ・・・？”

![imageVLM1-37-5](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-5.jpg)

D先生 ： “あれ？リモコンという分類結果が出てきましたね。まあ、いろいろな画像を学習しているので、こういうイレギュラーもあるんでしょうか・・・。”

QEU:FOUNDER ： “フン！気に入らん。俺の猫様を無視しやがって・・・。それでは、なぜリモコンを検出したのかについて、アテンション・マップを見て分析したいと思います。これからがアテンション・マップのためのプログラムです。”

```python
######################################
# ATTENTION MAP DRAWING
######################################
# ---
import torch
import requests
import numpy as np
import matplotlib.pyplot as plt
from transformers import ViTForImageClassification, ViTImageProcessor
from PIL import Image, ImageFilter

# ---
img_size_x = image.width
img_size_y = image.height
num_of_patches = 144

# Load a pre-trained ViT model and feature extractor
model_name = 'google/vit-large-patch32-384'
processor = ViTImageProcessor.from_pretrained(model_name, do_rescale=False)
model = ViTForImageClassification.from_pretrained(model_name, attn_implementation='eager')

# 画像のURLを指定
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

# Run an image through the pipline
inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs, output_attentions=True)

# Getting the attentions
attentions = outputs.attentions 

# ---
fig, axs = plt.subplots(4, 4, figsize=(20, 20))
for i, ax in enumerate(axs.flatten()):
    ax.imshow(attentions[-1][0, i, :, :].detach().cpu().numpy())
    ax.axis('off')
plt.show()

# ---
def attention_rollout(attentions):
    # Initialize rollout with identity matrix
    rollout = torch.eye(attentions[0].size(-1)).to(attentions[0].device)

    # Multiply attention maps layer by layer
    for attention in attentions:
        attention_heads_fused = attention.mean(dim=1) # Average attention across heads
        attention_heads_fused += torch.eye(attention_heads_fused.size(-1)).to(attention_heads_fused.device) # A + I
        attention_heads_fused /= attention_heads_fused.sum(dim=-1, keepdim=True) # Normalizing A
        rollout = torch.matmul(rollout, attention_heads_fused) # Multiplication

    return rollout

# ---
rollout = attention_rollout(attentions)
cls_attention = rollout[0, 1:, 0:2]  # Get attention values from [CLS] token to all patches
cls_attention = cls_attention.mean(dim=1).detach().cpu().numpy()
cls_attention = 1 - cls_attention.reshape(int(np.sqrt(num_of_patches)), int(np.sqrt(num_of_patches)))

# ---
fig = plt.figure(figsize=(12, 12))
plt.imshow(cls_attention)
plt.axis('off')
plt.show()

```

QEU:FOUNDER ： “GoogleのVITモデルでは、画像を認識したときのアテンションの情報を出力してくれます。それを画像でまとめました。右側の画像、グラフを見てください。”

![imageVLM1-37-6](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-6.jpg)

D先生 ： “右の画像は、見栄えがヒートマップみたいですね。・・・とすると、黄色の部分のアテンション値が大きいのでしょうね。画像と比較すると・・・。そこにあるのは、「リモコン」ですね。”

QEU:FOUNDER ： “このマップをみれば、なぜリモコンという分類結果がでたのかが説明できます。あとは、このヒートマップを入力画像に合わせるための処理をしましょう。”

```python
# ---
# Normalize the attention map for better visualization
cls_attention_temp = (cls_attention - cls_attention.min()) / (cls_attention.max() - cls_attention.min())

# Resize and blur the attention map
cls_attention_resized = Image.fromarray((cls_attention_temp * 255).astype(np.uint8)).resize((img_size_x, img_size_y), resample=Image.BICUBIC)
cls_attention_resized = cls_attention_resized.filter(ImageFilter.GaussianBlur(radius=2))
cls_attention_resized   # 画像出力

# ---
# Convert the attention map to RGBA
cls_attention_colored = np.array(cls_attention_resized.convert("L"))
cls_attention_colored = np.stack([cls_attention_colored]*3 + [cls_attention_colored], axis=-1)

# Adjust the alpha channel to control brightness
cls_attention_colored_img = Image.fromarray(cls_attention_colored, mode="RGBA")
#cls_attention_colored_img

# ---
# 画像を並列に表示
fig, axes = plt.subplots(1, 2, figsize=(16, 8))

axes[0].imshow(image)
axes[0].axis('off')  # 軸を非表示
axes[0].set_title('RAW PICTURE')

axes[1].imshow(cls_attention_colored)
axes[1].axis('off')  # 軸を非表示
axes[1].set_title('ATTENTION MAP')

```

QEU:FOUNDER ： “これで、十分に画像がみやすくなります。”

![imageVLM1-37-7](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-7.jpg)

D先生 ： “わかりやすくなりした。なるほど、このGoogle/ViTの新モデルが**ファインチューニング**できれば、おもしろくなりますね。”

QEU:FOUNDER ： “じゃあ、これからファインチューニングをつかって、外観検査ができるのかをやってみましょう。”

D先生 ： “おっと、おもしろくなってきた！！”

QEU:FOUNDER ： “最初のステップは、新しいデータセットの生成です。もちろん、Florence-2モデルでも使えるようにしたいですね。”



## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “この話（↓）も、同じ次元にある話なんでしょうね。ホント世の中は腐っていますね。上も、下も・・・。”

**（上の話）**

![imageVLM1-37-8](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-8.jpg)

**（下の話）**

![imageVLM1-37-9](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-9.jpg)

C部長 : “長い間の腐敗体制の結果でしょ？なにしろ**下部構造がこんな状態（↓）**です。これから、そう簡単によくなるのかなあ・・・。 “

[![MOVIE1](http://img.youtube.com/vi/AMYxPi8amA8/0.jpg)](http://www.youtube.com/watch?v=AMYxPi8amA8 "日本變成男盗女娼社會？中學生參與強盗案被逮捕 為了交稅而走上犯罪之路的青年！")

QEU:FOUNDER ： “長い時間の問題の蓄積の結果ですが、原因の一部は明らかになっています。”

![imageVLM1-37-10](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-10.jpg)

C部長 : “ただし、これが最も大きな原因であるとは、思わんけどなあ・・・。“

![imageVLM1-37-11](/2024-10-24-QEUR23_VIVLM37/imageVLM1-37-11.jpg)

QEU:FOUNDER ： “ただし、**世直しの大前提**には、なるんじゃないの？”
