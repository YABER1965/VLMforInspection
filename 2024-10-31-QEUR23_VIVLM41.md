---
title: QEUR23_VIVLM41 – アテンションマップを作画する
date: 2024-10-31
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "Vision language Model"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VIVLM41 – アテンションマップを作画する

## ～ 外観検査に向いているViTモデルがわかった！ ～

QEU:FOUNDER ： “いままでのところ、EPOCH数を5回まで学習しました。その分類パフォーマンスは、confusion matrixで以下の通り（↓）です。これから、このモデルを使ってアテンションマップを作成したいが・・・。”

![imageVLM1-41-1](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-1.jpg)

D先生 ： “FOUNDER・・・。このパフォーマンスでは、全然だめだと思います。せめて、EPOCH数を増やしてください。”

QEU:FOUNDER ： “え～。まだコストをかけるのか・・・。しようがない。ＥＰＯＣＨ数を10まで上げてみましょう。”

![imageVLM1-41-2](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-2.jpg)

D先生 ： “やってよかったでしょ？すくなくともX方向の欠陥なのか、Y方向の欠陥なのかが明確に判別できるようになっています。それにしても、合格(passed)の判別レベルが上がりませんね。”

QEU:FOUNDER ： “合格の学習データの情報が少なすぎるんですよ。すくなくとも、全体のデータ量の1/5は必要です。”

D先生 ： “いまは、どれぐらいの合格データがあるんですか？”

QEU:FOUNDER ： “**全体の1/50位しかない**です。まあ、しようがない。このデータセットはそもそもFlorence-2用で設計したのだから・・・。さて、このような問題点をあらかじめ理解したうえでアテンションマップを描いてみましょう。ここで、マップ作図用のプログラムは以前に紹介したので、今回はファインチューンしたモデルで描いた画像だけを紹介します。まずは猫様（↓）から・・・。”

![imageVLM1-41-3](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-3.jpg)

D先生 ： “なんか、ずいぶん角ばったマップになってしまいました。分類予測した結果はどうでしたか？「猫」と出力されましたか？”

QEU:FOUNDER ： “猫のかわりに欠陥情報が出力されました。すでにモデルがファインチューニングされていますからね。ちなみに、pre-trainingモデルでは、猫様と出力されます。だから、アテンションマップの「模様」が猫の上にあるんです。次に、製品（欠陥）画像のマップを見てみましょう。”

![imageVLM1-41-4](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-4.jpg)

D先生 ： “えっ！？こんなかんじなの？全然だめじゃないですか・・・。”

QEU:FOUNDER ： “本来は、欠陥位置に模様が配置されていなければいけないですよね。一応、正解の答えが出ていましたが、**「何を見て判断したの？」という疑問**がつきます。もう一例を見てみましょう。”

![imageVLM1-41-5](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-5.jpg)

D先生 ： “あれえ！？NSOARTC処理をしない原画像の方が、アテンションの配置が合理的ではないですか？”

QEU:FOUNDER ： “そもそも、NSOARTC画像のような合成画像はモデルの中にはないんですよ。最後にもう一件を見てみましょう。”

![imageVLM1-41-6](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-6.jpg)

D先生 ： “これも同じですね。原画像のほうがマップが合理的です。う～ん・・・。どうしてこうなったのだろう。前回の外観検査自動機のときは、非常にきれいなアテンションマップが得られました。”

![imageVLM1-41-7](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-7.jpg)

QEU:FOUNDER ： “前回の学習データの方が、角形端子なども入って学習しやすくなっています。学習量もおおきかったです。でも、もっとも大きな違いはモデルの違いじゃないでしょうか。これは、前回に成功したプロジェクトで使用したViTモデルの構造です。”

![imageVLM1-41-8](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-8.jpg)


D先生 ： “学習可能なパラメタが16万しかないんですか？すごく小さなモデルですね。じゃあ、今回のGOOGLE/VITモデルはどのようになっていますか。”

QEU:FOUNDER ： “じゃあ、これから専用のプログラムを作って調べてみましょう。小生は、この学習によってモデルのどの程度のパラメタが書き換わるのかについて、前々から興味がありました。”

```python
# ----
from transformers import ViTForImageClassification
import torch

# ViTモデルのロード
model_name = 'google/vit-large-patch16-224'
model = ViTForImageClassification.from_pretrained(model_name)

# 全パラメータの数
total_params = sum(p.numel() for p in model.parameters())

# 学習可能なパラメータの数（書き換えられるパラメータ）
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# 結果の表示
print(f'全パラメータ数: {total_params}')
print(f'学習可能なパラメータ数: {trainable_params}')
print(f'ファインチューニングで書き換えるパラメータの割合: {trainable_params / total_params * 100:.2f}%')

```

QEU:FOUNDER ： “そして、計算結果をドン！！”

![imageVLM1-41-9](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-9.jpg)

D先生 ： “えっ！そうなの？（モデルの）パラメタを全部書き換え！？これでは**ファインチューン（転移学習）じゃない**です。それにしても、このモデルは**3億個もパラメータがある**のか・・・。我々の合成画像を認識するモデルとしては、ちょっと「贅沢すぎる」ようですね。”

QEU:FOUNDER ： “今回のトライアルで、ある種の結論が出たような気がします。合成画像を使った外観検査の場合、インプットの段階でかなりの処理がされています。この場合には、**ハイスペックなViTモデルが良いとは、必ずしも言えない**んですよ。”

D先生 ： “Google/Vitモデルのアテンションマップの模様も、ちょっと変ですからね。あと、もう一つ質問です。ハイスペックなモデルが外観検査に不利であるならば、さらに複雑なFlorence-2は外観検査に向くのですか？”

![imageVLM1-41-10](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-10.jpg)

QEU:FOUNDER ： “前回のLoraのトライアルで、「Florence-2のファインチューニングは難しいな」と感じていました。他の人も、同じことを考えているようです。物体検出は**マルチタスクで複雑**なんですよ。一つの命令で、分類、位置検出、大きさ検出の3つの情報を出力しなければなりません。Loraでモデルの一部分だけ変えれば、全体のバランスが変わてしまって、パフォーマンスが悪くなるんです。”

![imageVLM1-41-11](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-11.jpg)

D先生 ： “その考え方はおかしいです。じゃあ、なぜこの事例（↑）では成功したのですか？”

QEU:FOUNDER ： “Pre-trainモデルの状態で、すでにトランプのカードはたくさん学習されているんです。ただし、そのPre-trainモデルが想定する用途の中では**「ダイヤのエース」**とかいうような、より細かい分類がなかっただけです。その不足部分をファインチューニングで追加したんです。”

D先生 ： “なるほどねえ・・・。逆に言うと、Florence-2を推論として使うのは良いとして、このモデルをファインチューニングして使うことは、**人工物の検査ではやめた方がいい**ですね。”

QEU:FOUNDER ： “モデルのパラメタの半分以上を変えてしまうか、それともPre-trainモデルの開発時に、そのようなユース・ケース用のデータを追加してもらうしかないでしょうね。はっきりいって、特別の設備やスキルを持たないユーザーが学習して、よいパフォーマンスを出すには、Florence-2は複雑すぎるように感じます。”

D先生 ： “じゃあ、外観検査自動機の延長線のプロジェクトはこれで終わり？”

QEU:FOUNDER ： “漏れている問題点はないかは、今後も継続して検討します。基本的にはここまでです。非常に多くの知見をえられました。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “国家経済の根本問題は、**「われわれ（自国）と外部世界との交易がどのようになっているか」**です。自由な国際貿易が確立された現在では、戦争による植民地支配をする必要がないんです。貿易を通じて搾取すればいいわけで・・・。”

![imageVLM1-41-12](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-12.jpg)

C部長 : “「搾取」って物騒な・・・。“

[![MOVIE1](http://img.youtube.com/vi/FCc8cojW3ZU/0.jpg)](http://www.youtube.com/watch?v=FCc8cojW3ZU "田内学×宮台真司：人を幸せにする経済とは")

QEU:FOUNDER ： “「貿易を通じた搾取」って表現は、別に大げさじゃないよ。これは、**石橋湛山の「小日本主義」の考え方の根本**です。現に、リーマンのときにバルカンのG国が国家破産したでしょ？もともと、G国の主要産業って、当時は**インバウンド（観光）**と、海運、オリーブぐらいしかなかったじゃないですか・・・。”

C部長 : “インバウンド・・・。嫌な予感・・・。“

![imageVLM1-41-13](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-13.jpg)

QEU:FOUNDER ： “インバウンドって、**「国際交易の本土決戦」**なんですよ。なんでまあ、J国は30年もかけて、本来の勝ち戦から本土決戦まで見事な失敗連敗を続けたんだろうか・・・。そのヒントの一つが、この、小生が愛してやまない**「エズさん」の2冊の本（↑）**にあります。”

C部長 : “よくもまあ、FOUNDERは、こんな「マイナーな人の本」に目をつけましたねえ・・・。 “

![imageVLM1-41-14](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-14.jpg)

QEU:FOUNDER ： “マイナーでもないよ。J国では**「あのネトウヨのアイドル」**によって翻訳されているんだから・・。そうだった・・・。小生には非常にかすかな記憶があったんだ・・・。どうだろう。2002年ごろかなあ・・・。洋書店を歩いていると、あるJ国首相がカバーとなった雑誌があったんです。TIME? NEWSWEEK? ECONOMIST？名前は忘れました・・・。ただ、印象に残っていたのが、あの「アンちゃんのお父さん（↑）」の写真と上に「KAWARI」って書いてあったんです。”

C部長 : “J語でいうと、**「KAWARI」**って、ちょっと変な感じですよね。 だから印象に残ったのでしょうね。“

QEU:FOUNDER ： “ず～っと長い間、そんな変な潜在意識を持っていて、ある日、図書館でこの本を見つけたんです。そして、その本を読んでみて、ひっくり返るほどおどろいた！！”

C部長 : “「忘れたようで忘れない」というFOUNDERの性格の悪さが出ています。で？その本を読んだのは、いつごろのことですか？ “

QEU:FOUNDER ： “QEUがプロジェクトとして始まったころだから、2011年前後だったかなあ・・・。これを読んで驚いたのなんのって・・・。この本の言いたいことは、**「21世紀のJ国のあるべき姿は、自国の工場はなるべく外国に出し、自国ではインバウンドに集中すべき」**なんです。”

C部長 : “どういうロジックで、こんな変な結論になるんですか？ “

QEU:FOUNDER ： “たしか、第1章で書いてあったと思います。一つは、J国は前の戦争で迷惑をかけたこと。もう一つの要因は、少子高齢化です。”

C部長 : “少子高齢化への対策がそれでは、世界のほとんどの国は同じ対策を取るべきという結論になりますよ。 “

![imageVLM1-41-15](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-15.jpg)

QEU:FOUNDER ： “わすれちゃったなあ。小生の愛するあのお方が、実際のところ、どのように考えているのか・・・。その後に読んだ**「THIRTY TOMORROW」**でも驚きました。彼の訴えていることは、少子高齢化において**「老人は〇ぬまで働け」**です。”

![imageVLM1-41-16](/2024-10-31-QEUR23_VIVLM41/imageVLM1-41-16.jpg)

QEU:FOUNDER ： “それに比べれば、「ケルトンさんの理屈」って、とてもかわいいものですよ。実際のところ、あの頃（21世紀初頭）、この理屈をうまく使いこなせばJ国は、実際に成長したと思いますよ。”

### 生産的投資: 政府支出が生産的投資 (インフラ、教育など) に向けられれば、経済力と成長の可能性を高めることができます。

C部長 : “・・・と、元ネトウヨのFOUNDERが申しております・・・（笑）。“

[![MOVIE2](http://img.youtube.com/vi/SoAsEOWdJZQ/0.jpg)](http://www.youtube.com/watch?v=SoAsEOWdJZQ "海外串流平台要杯葛日本動畫？人工低工時長廉價血汗動畫師 月入只有五千港幣？")

QEU:FOUNDER ： “あのころ、ちゃんとした国造りをしていれば、現在の経済の主柱の一つである**コンテンツ産業でさえもが搾取によって成り立つ**ような情けない状況になっていなかったと思いますよ。”

