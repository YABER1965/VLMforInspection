---
title: QEUR23_VIVLM32 : Florence2でfinetuneをOD用のfinetuneをしてみる
date: 2024-10-14
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "Vision language Model"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VIVLM32 : Florence2でfinetuneをOD用のfinetuneをしてみる

## ～ やっぱり、Florence2の方がいいかも ～

QEU:FOUNDER ： “ここで方針を変更します！今回は、Florence（以下FL）を使ったOD(Object Detection:物体検出）をやりましょう。”

![imageVLM1-32-1](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-1.jpg)

D先生 ： “あれ？ TU国の成果物（↓）を使うんじゃなかったのですか？”

![imageVLM1-32-2](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-2.jpg)

QEU:FOUNDER ： “MODEL CARDにfinetune用のnotebookがあったんです。でも、なんでかなあ・・・、うまく動かないです。・・・というか、コードを見てもいくつか矛盾があり、そもそもアレは動くのかなァ・・・。Florence-2で調査すると、**「ちゃんと動くたたき台notebookがあった」**ので、それを改造して使います。”

![imageVLM1-32-3](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-3.jpg)

QEU:FOUNDER ： “さて、例のTraVisionLMと同一の作者なのだが、彼はなかなか良いVLM用のデータセットを準備しているんです。もちろん、コメントはTU語を使っているのだが・・・。”

![imageVLM1-32-4](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-4.jpg)

D先生 ： “でも、FOUNDERは、その優秀なデータセットをわざわざ書き換えたんですよね。なぜですか？”

QEU:FOUNDER ： “そのCOCOデータセットは何しろ大きすぎます。小生の使用目的は大げさなものではないです。より低コストでfinetuningができるように大幅に削りました。あとは、bbox_strのコメントの構造が変わっています。さて、finetuningのプログラムを順番に見てみましょう。”

```python
# ---
from datasets import load_dataset

dataset = load_dataset("YxBxRyXJx/cut_TRV_train_10116")
#dataset["train"][0]

# ---
import torch

# ---
# データセットには 「bbox_str」フィールドに含まれるようになります。このフィールドを予測するために、Florence-2 モデルを微調整します。
# Let's create train and validation splits.
split_dataset = dataset["train"].train_test_split(test_size=0.2, shuffle=True)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]
print("Len train dataset: ", len(train_dataset))
print("Len eval dataset: ", len(eval_dataset))
# ---
test_dataset = dataset["test"]
print("Len test dataset: ", len(test_dataset))

```

QEU:FOUNDER ： “ここまでで、我々の作ったデータセットが無事に読み込ませることが出来ました。”

![imageVLM1-32-5](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-5.jpg)

D先生 ： “なんか、データ数は端数になっていますよね。（データを）削りましたか？”

QEU:FOUNDER ： “**白黒画像を削りました**。Finetuneで途中でエラーが出たので、原因を調べてみたら白黒画像でした。じゃあ、モデルを読み込みましょう。”

```python
# ---
# Load Model and Processor
from transformers import AutoProcessor, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("microsoft/Florence-2-large-ft", revi-sion="refs/pr/10", trust_remote_code=True, device_map="cuda") # load the model on GPU
processor = AutoProcessor.from_pretrained("microsoft/Florence-2-large-ft", revision="refs/pr/10", trust_remote_code=True)

```

D先生 ： “このモデルとデータセットを使って、画像表示をしてみたんですね。”

```python
# ---
# Let's define a plotting function to show the detected objects in a given image with an example from the training dataset.
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# ---
def run_example(task_prompt, image, max_new_tokens=128):
    prompt = task_prompt
    inputs = processor(text=prompt, images=image, return_tensors="pt")
    generated_ids = model.generate(
      input_ids=inputs["input_ids"].cuda(),
      pixel_values=inputs["pixel_values"].cuda(),
      max_new_tokens=max_new_tokens,
      early_stopping=False,
      do_sample=False,
      num_beams=3,
    )
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    parsed_answer = processor.post_process_generation(
        generated_text,
        task=task_prompt,
        image_size=(image.width, image.height)
    )
    return parsed_answer

# ---
def plot_bbox(image, data):
   # Create a figure and axes
    fig, ax = plt.subplots()

    # Display the image
    ax.imshow(image)

    # Plot each bounding box
    for bbox, label in zip(data['bboxes'], data['labels']):
        # Unpack the bounding box coordinates
        x1, y1, x2, y2 = bbox
        # Create a Rectangle patch
        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')
        # Add the rectangle to the Axes
        ax.add_patch(rect)
        # Annotate the label
        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))

    # Remove the axis ticks and labels
    ax.axis('off')

    # Show the plot
    plt.show()

# ---
example_id = 250
image = train_dataset[example_id]["image"]
#image

# notice here that <OD> task prompt is used. This task prompt is already used in training the Flor-ence-2 model checkpoints for object detection.
parsed_answer = run_example(task_prompt="<OD>", image=image)
plot_bbox(image, parsed_answer["<OD>"])

```

QEU:FOUNDER ： “Pre-trainの状態のモデルの出来栄えを見てみましょう。”

![imageVLM1-32-6](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-6.jpg)

D先生 ： “あれ？Florence-2へ供給するプロンプトはどこに？”

QEU:FOUNDER ： “TraVisionLMモデルとは違い、**Florence-2モデルのプロンプトは「<OD>」です**。これは、「Object Detectionをしてくれ」という意味です。それでは、コードをつづけます。”

```python
# ----
# トレーニング中の OOM エラーを防ぐために、ビジュアル エンコーダー レイヤーをフリーズしてメモリを節約しています。Florence-2 にはオブジェクト検出機能がすでに備わっているため、完全なモデルを微調整するのではなく、トランスフォーマー エンコーダー/デコーダー レイヤーのみをトレーニングすることをお勧めします。より強力なリソースがある場合や、モデルにさまざまなタスクを実行させたい場合は、この設定を自由に検討してください。
for param in model.vision_tower.parameters():
  param.requires_grad = False

model_total_params = sum(p.numel() for p in model.parameters())
model_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Number of trainable parameters {model_train_params} out of {model_total_params}, rate: {model_train_params/model_total_params:0.3f}")

```

QEU:FOUNDER ： “これ（↓）をみると、今回の訓練対象のパラメタ数がわかります。”

![imageVLM1-32-7](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-7.jpg)

D先生 ： “えっ！？モデルの半分以上も変えるんですか！？”

QEU:FOUNDER ： “小生も、ここまでくると（計算時間が）無駄だと思うんだけどなあ・・・。10％以下に抑えてほしいところです。次に行きましょう。いよいよトレーニングです。”

```python
# ---
IGNORE_ID = -100 # Pytorch ignore index when computing loss
MAX_LENGTH = 512

def collate_fn(examples):
    task_prompt = "<OD>"

    prompt_texts = [task_prompt for _ in examples]
    label_texts = [example["bbox_str"] for example in examples]
    images = [example["image"] for example in examples]

    inputs = processor(
        images=images,
        text=prompt_texts,
        return_tensors="pt",
        padding="longest",
        max_length=MAX_LENGTH,
    )

    labels = processor.tokenizer(
        label_texts,
        return_tensors="pt",
        padding="longest",
        max_length=MAX_LENGTH,
        return_token_type_ids=False, # no need to set this to True since BART does not use token type ids
    )["input_ids"]

    labels[labels == processor.tokenizer.pad_token_id] = IGNORE_ID # do not learn to predict pad to-kens during training

    return_data = {**inputs, "labels": labels}
    return return_data

# Test the data collator.
collated_examples = collate_fn([train_dataset[0], train_dataset[6]])
#collated_examples

# ---
# 以下に設定されているトレーニング パラメータを自由に調整してください。Florence-2 のような小さなモデルは、微調整中にトレーニング データに簡単に過剰適合する可能性があるため、比較的低い学習率が使用されることに注意してください。
# ---
from transformers import TrainingArguments

args=TrainingArguments(
    output_dir="drive/MyDrive/FTmodels/Florence-2-OD-COCO",
    num_train_epochs=2,
    learning_rate=3e-6,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy="epoch",
    logging_strategy="epoch",
    eval_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=False, # we will manually push model to the hub at the end of training
    label_names=["labels"],
    remove_unused_columns=False,  # needed for data collator
)
# ---
from transformers import Trainer
trainer = Trainer(
    model=model,
    tokenizer=processor,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=collate_fn, # dont forget to add custom data collator
    args=args
)
# ---
trainer.train()

```

D先生 ： “学習は、たった２エポックなんですか？”

![imageVLM1-32-8](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-8.jpg)

QEU:FOUNDER ： “時間がかかるのね、コレ・・・。もともとテスト目的なので2回で十分です。じゃあ、最後にモデル推論の出来栄えを見てみましょう。”

```python
# ---
# 検証セットの例を使用して、微調整したモデルを評価してみましょう。プロット関数と生成関数はすでに定義されています。
example_id = 100
image = test_dataset[example_id]["image"]
parsed_answer = run_example("<OD>", image=image)
plot_bbox(image, parsed_answer["<OD>"])

```

D先生 ： “キャプションの「insan」って、何ですか？”

![imageVLM1-32-9](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-9.jpg)

QEU:FOUNDER ： “TU語で「人」という意味です。とりあえず、今回のトライアルでfinetuning のプログラムが動くことが検証されました。いよいよ、外観検査自動機用のNSOARTC画像のデータを作りましょう。”

![imageVLM1-32-10](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-10.jpg)

D先生 ： “例によって、「ありあわせのデータ」でやるんでしょ？”

QEU:FOUNDER ： “学習データって、1000件あればいい方じゃないかな？”

D先生 ： “LoRAが使えればよかったのにね・・・。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “さっき、選挙の立候補予定者一覧を長い間見てたんだけど、もう面白くってしかたがない。小生の第一印象もこの人（↓）と同じですよ。ちょっと、出馬の乱雑さがひどすぎ・・・。今回は、下部構造の本質的な変化が引き起こす、**国家構造の歴史的な大変化の一歩手前**なんじゃないでしょうか・・・。”

![imageVLM1-32-11](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-11.jpg)

QEU:FOUNDER ： “面白いんだよね。A国で起こっていることは、J国とほとんど同じなんだよね。政治家になりたがるエリートは、(自分の)お金儲けのことしか考えておらず、理念を持っていない。”

[![MOVIE1](http://img.youtube.com/vi/TI1ivRzF1SA/0.jpg)](http://www.youtube.com/watch?v=TI1ivRzF1SA "恐慌・パンデミック・戦争…21世紀読み解きと世界の行方")

C部長 : “A国の2大政党制は、昔はリベラルと保守と言われました。いまは、ほとんど同じです。むしろ、いわゆるリベラル側が狂暴化しています。“

QEU:FOUNDER ： “狂暴化？物騒な言葉を・・・。”

![imageVLM1-32-12](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-12.jpg)

C部長 : “そうねえ、狂暴という表現が妥当だねえ・・・。なんとかならないものか・・・。 “

![imageVLM1-32-13](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-13.jpg)

QEU:FOUNDER ： “このスーパーマン（↑）に、中東問題の解決をやってもらいましょう！”

C部長 : “は？な・・・なにをやってもらう！？それにしても、**経済問題を前提にして尊厳〇を議論する**の？ひどいなあ・・・。 “

![imageVLM1-32-14](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-14.jpg)

C部長 : “へえ・・・。**こういう行為を「姥捨山」という**のか・・・。お若いのに物知りな人ね💛とても勉強になったわ・・・。なんだ、**「複数の人たち」がこんなスゴイことを、まじめに前向きに議論している**わけですね。 “

![imageVLM1-32-15](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-15.jpg)

QEU:FOUNDER ： “なんか、政党全体がそういう意見のようですね。「高齢者には尊厳〇」なんでしょ？そう遠くない距離に「身体障碍者には尊厳〇」があります。もうちょっと離れた位置に、「ADHDには尊厳〇」という議論があります。”

C部長 : “ずんずん議論すると、**「オレ様の気に入らない奴には尊厳〇」という結論に着地する**でしょう。ああ、**経済が前提の尊厳〇議論**というのは、いやだなあ・・・。 だけど、なぜこの人が中東問題を解決するの？“

![imageVLM1-32-16](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-16.jpg)

QEU:FOUNDER ： “国民民〇党のおじさん、とっても首相になりたいんでしょ？じゃあ、あの国にいって首相になってもらいましょう。あそこの人たち、みんなショックを受けますよ。**「自国内で自分をジェノサイドする奴が現れた」**って・・。”

C部長 : “**「We are pushing self-genxcxde in our country!!」**・・・。さすがに、あの国の中の人たちも、自分のｱﾎさに目が覚めると思います。“

![imageVLM1-32-17](/2024-10-14-QEUR23_VIVLM32/imageVLM1-32-17.jpg)

QEU:FOUNDER ： “今回の選挙はエキサイティングだなあ・・・。与党よりも、むしろ野党の方が相当変わると思うわ・・・。”

