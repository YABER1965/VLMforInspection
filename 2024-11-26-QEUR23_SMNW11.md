---
title: QEUR23_SMNW11 – UnslothでVLMを学習してみる
date: 2024-11-26
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_SMNW11 – UnslothでVLMを学習してみる

## ～ ここまでは、「思ったよりも良い」といえる。 ～

### ・・・ 学習はほんの「序盤」です。 ・・・

D先生 ： “推論(inference)のトライアルがおわり、いよいよ「学習」が始まります。今回のトライアルで「一発OK！（垂直立ち上げ）」になると思いますか？”

![imageSMNW3-2-1](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-1.jpg)

C部長 ： “そういえば、以前、「VLM(Vision Language Model)のファインチューニングは難しいからなあ・・・」と、言っていましたね。なぜ、そう思ったのですか？”

QEU:FOUNDER ： “あまり詳しい情報は知らないんだが、VLMって、もともとのLLMの構造の外側にVisualの要素をくっつけたんでしょ？もし、それが本当であるならば、Pre-train段階でかなり「アクロバット」な学習によって辻褄を合わせたシロモノであって、あとでユーザーが適当にファインチューニングができるのであるかと・・・。あと、転移学習というのは、あくまでLLM側に対応しているのであって、Visual側は、本当に学習するのかが疑問・・・。”

D先生 ： “そういう意味では、「VLM」モデル（↓）によって、推論パフォーマンスが相当かわる可能性がありますね。”

```python
# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit", # Llama 3.2 vision support
    "unsloth/Llama-3.2-11B-Vision-bnb-4bit",
    "unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit", # Can fit in a 80GB card!
    "unsloth/Llama-3.2-90B-Vision-bnb-4bit",

    "unsloth/Pixtral-12B-2409-bnb-4bit",              # Pixtral fits in 16GB!
    "unsloth/Pixtral-12B-Base-2409-bnb-4bit",         # Pixtral base model

    "unsloth/Qwen2-VL-2B-Instruct-bnb-4bit",          # Qwen2 VL support
    "unsloth/Qwen2-VL-7B-Instruct-bnb-4bit",
    "unsloth/Qwen2-VL-72B-Instruct-bnb-4bit",

    "unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit",      # Any Llava variant works!
    "unsloth/llava-1.5-7b-hf-bnb-4bit",
] # More models at https://huggingface.co/unsloth

```

QEU:FOUNDER ： “そうです。幸いなことに、Unslothは多くのモデルをサポートしています。今回は、贅沢にも、ある程度ベンチマークプログラムが完成したら、「llama-Pixtral-Qwen-llava」の代表モデルでやってみたいです。D先生は、どのモデルが有望と見ていますか？”

D先生 ： “なにしろ、今回の外観検査の情報そのものはVLMの学習データには無いものです。万が一、**あるモデルが「今回と近いケース」を学習していたら、パフォーマンスが突出して高くなる**でしょうね。”

QEU:FOUNDER ： “それでは、プログラムを動かしましょう。まずは序盤から・・・。”

```python
# ---
# train dataset
from datasets import load_dataset
dataset = load_dataset("YxBxRyXJx/CYLNimages_train_1015")["train"]

# ---
# test dataset
#from datasets import load_dataset
dataset_test = load_dataset("YxBxRyXJx/CYLNimages_test_1015")["train"]

# ---
import torch
from unsloth import FastVisionModel # FastLanguageModel for LLMs

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit", # Llama 3.2 vision support
    "unsloth/Llama-3.2-11B-Vision-bnb-4bit",
    "unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit", # Can fit in a 80GB card!
    "unsloth/Llama-3.2-90B-Vision-bnb-4bit",

    "unsloth/Pixtral-12B-2409-bnb-4bit",              # Pixtral fits in 16GB!
    "unsloth/Pixtral-12B-Base-2409-bnb-4bit",         # Pixtral base model

    "unsloth/Qwen2-VL-2B-Instruct-bnb-4bit",          # Qwen2 VL support
    "unsloth/Qwen2-VL-7B-Instruct-bnb-4bit",
    "unsloth/Qwen2-VL-72B-Instruct-bnb-4bit",

    "unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit",      # Any Llava variant works!
    "unsloth/llava-1.5-7b-hf-bnb-4bit",
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastVisionModel.from_pretrained(
    "unsloth/Llama-3.2-11B-Vision-Instruct",
    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for long context
)

model = FastVisionModel.get_peft_model(
    model,
    finetune_vision_layers     = True, # False if not finetuning vision layers
    finetune_language_layers   = True, # False if not finetuning language layers
    finetune_attention_modules = True, # False if not finetuning attention layers
    finetune_mlp_modules       = True, # False if not finetuning MLP layers

    r = 16,           # The larger, the higher the accuracy, but might overfit
    lora_alpha = 16,  # Recommended alpha == r at least
    lora_dropout = 0,
    bias = "none",
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
    # target_modules = "all-linear", # Optional now! Can specify a list if needed
)

# ---
instruction_DST = "You are a visual inspector who judges whether a product is passed or defective. Both image1 and image2 show a group of pins (cylinders) on the product. These images show pins from above. Image2 should have the same pin quantity and arrangement as image1. First, you should count the number of pins on image2. Image1 is a image of passed product. Please compare all of pins on the product one by one. Any significant difference on pin on the same address is defined as defect. After your inspection, you should describe type of image, number of pins, defect mode, and defect position. Please note that if you find a large white circle nearby pin, it is likely to be defect."
# ---
instruction_RAW = "You are a visual inspector who judges whether a product is passed or defective. Both image1 and image2 show a group of pins (cylinders) on the product. These images show pins from above. Image2 should have the same pin quantity and arrangement as image1. First, you should count the number of pins on image2. Image1 is a image of passed product. Please compare all of pins on the product one by one. Any significant difference on pin on the same address is defined as defect. After your inspection, you should describe type of image, number of pins, defect mode, and defect position."
print(instruction_RAW)

# -----
# VLM Inference
import numpy as np
import random
from PIL import Image
from IPython.display import Markdown, display
import matplotlib.pyplot as plt

# -----
# 作画関数(for 2pic)
def draw_two_pic(str_title1, arr_image1, str_title2, arr_image2):
    # グラフ化
    fig = plt.figure(figsize=(10,10))
    fig.tight_layout(rect=[0,0,1,0.96])
    # ---
    plt.subplot(1,2,1)
    plt.title(str_title1)
    plt.imshow(arr_image1)
    # ---
    plt.subplot(1,2,2)
    plt.title(str_title2)
    plt.imshow(arr_image2)
    # 画像の表示
    plt.show()

# ---
def convert_to_message(sample):
    # ----
    str_dir = sample["dir"]
    str_position = sample["position"]
    str_type = sample["type"]
    str_bbox = sample["bbox_str"]
    # ----
    instruction = "NA"
    if str_type == 'CYLN_DST':
        instruction = instruction_DST
    else:
        instruction = instruction_RAW
    # ----
    messages = [
        { "role": "user",
          "content" : [
            {"type" : "text",  "text"  : instruction},
            {"type" : "image", "image" : "image1"},
            {"type" : "image", "image" : "image2"} ]
        }
    ]
    # ----
    index_bbox = str_bbox.index("<loc")
    str_defect = str_bbox[:index_bbox]
    str_defective = "passed"
    if str_defect != "passed":
        str_defective = "defective"
    # ----
    str_answer = f"- image type->{str_type}\n- number of pins->27 pins\n- your judgement->{str_defective}\n- defect mode->{str_defect}\n- defect position->{str_position}"

    return messages, instruction, str_answer

# ------
# 標準ファイル名(DST,RAW)
arr_fileimage_DST = ['NA' for i in range(6)]
arr_fileimage_RAW = ['NA' for i in range(6)]
arr_fileimage_DST[0] = 'camera2_0_454_499_89_0_0_OK_DST.jpg'
arr_fileimage_DST[1] = 'camera2_2_433_489_89_0_0_OK_DST.jpg'
arr_fileimage_DST[2] = 'camera2_2_454_499_89_0_0_OK_DST.jpg'
arr_fileimage_DST[3] = 'camera3_0_318_427_89_0_0_OK_DST.jpg'
arr_fileimage_DST[4] = 'camera3_0_487_486_89_0_0_OK_DST.jpg'
arr_fileimage_DST[5] = 'camera3_2_318_427_89_0_0_OK_DST.jpg'
# ---
arr_fileimage_RAW[0] = 'camera2_0_454_499_89_0_0_OK.png'
arr_fileimage_RAW[1] = 'camera2_2_433_489_89_0_0_OK.png'
arr_fileimage_RAW[2] = 'camera2_2_454_499_89_0_0_OK.png'
arr_fileimage_RAW[3] = 'camera3_0_318_427_89_0_0_OK.png'
arr_fileimage_RAW[4] = 'camera3_0_487_486_89_0_0_OK.png'
arr_fileimage_RAW[5] = 'camera3_2_318_427_89_0_0_OK.png'

# ---
def read_imagefile(arr_fileimage_DST, arr_fileimage_RAW, str_type):
    # ---
    str_dir = "drive/MyDrive/passed_image/"
    if str_type == 'CYLN_DST':
        filename = random.choice(arr_fileimage_DST)
        image = Image.open(str_dir + filename) 
    else:
        filename = random.choice(arr_fileimage_RAW)
        image = Image.open(str_dir + filename).convert("RGB")

    return image, filename

# ---
# データ番号(icount)を指定すること
icount = 0
sample = dataset_test[icount]
image1, filename = read_imagefile(arr_fileimage_DST, arr_fileimage_RAW, da-taset_test[icount]["type"])
image2 = dataset_test[icount]["image"]
# ---
message, instruction, str_answer = convert_to_message(sample)

# ---
#print("--- IMAGE(1) ---")
# 画像をarrayに変換
img_measure1 = np.asarray(image1)
str_title1 = f"Image1 - {filename}"
# ---
#print("--- IMAGE(2) ---")
# 画像をarrayに変換
img_measure2 = np.asarray(image2)
str_title2 = f"Image2 - DATA NO:{icount}"
# ---
draw_two_pic(str_title1, img_measure1, str_title2, img_measure2)

# ---
FastVisionModel.for_inference(model) # Enable for inference!

# ---
images = [image1, image2]
input_text = tokenizer.apply_chat_template(message, add_generation_prompt = True)
inputs = tokenizer(
    images,
    input_text,
    add_special_tokens = False,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)
response = tokenizer.batch_decode(outputs)
# ---
split_string = '<|start_header_id|>assistant<|end_header_id|>\n\n'
index_response = response[0].index(split_string)
cut_response = response[0][index_response+len(split_string):]
cut_response = cut_response.replace("<|eot_id|>","")
#print(cut_response)
print("--- INSTRUCTION ---")
display(Markdown(f"<b>{instruction}</b>"))
print("--- AI RESPONSE ---")
display(Markdown(f"<b>{cut_response}</b>"))
print("--- DATASET ANSWER ---")
display(Markdown(f"<b>{str_answer}</b>"))
```

D先生 ： “すいません。まずは、**プロンプト(instruction)**に注目したいです。”

You are a visual inspector who judges whether a product is passed or defective. Both image1 and im-age2 show a group of pins (cylinders) on the product. These images show pins from above. Image2 should have the same pin quantity and arrangement as image1. First, you should count the number of pins on image2. Image1 is a image of passed product. Please compare all of pins on the product one by one. Any significant difference on pin on the same address is defined as defect. After your in-spection, you should describe type of image, number of pins, defect mode, and defect position. Please note that if you find a large white circle nearby pin, it is likely to be defect.

あなたは、製品の良否を判断する目視検査員です。画像 1 と画像 2 はどちらも、製品上のピン (シリンダー) のグループを示しています。これらの画像は、ピンを上から見たものです。画像 2 は、画像 1 と同じピンの数と配置になっている必要があります。まず、画像 2 のピンの数を数えてください。画像 1 は合格製品の画像です。製品上のすべてのピンを 1 つずつ比較してください。同じアドレスのピンに大きな違いがある場合は、欠陥と定義されます。検査後、画像の種類、ピンの数、欠陥モード、欠陥の位置を記述する必要があります。ピンの近くに大きな白い円が見つかった場合は、欠陥である可能性が高いことに注意してください。

D先生 ： “うへえ・・・。複雑なプロンプトだなあ・・・。しかも、画像のタイプで切換・・・。”

QEU:FOUNDER ： “実際には、まだ指示の量が足らないと思います。ただし、指示の量を増やしたら、パフォーマンスがなぜか落ちたんです。・・・ともかく、ここまでの推論の結果を見てみましょう。”

![imageSMNW3-2-2](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-2.jpg)

C部長： “「AI RESPONSE」というのが、今回、AIモデルが出力した回答ですね。その一方で、データベースに記録している情報は「DATASET ANSWER」なんですね。当たり前ですが、RESPONSEの内容は「ズタボロ」ですね。まあ、ユーザーが期待していることを知らないのだから、当たり前なんですが・・・。”

D先生 ： “ピンの数の出力の質が、もう「ダメダメ」ですね。”

QEU:FOUNDER ： “英数字もピンとカウントしているようだね。じゃあ、学習を始めましょう。プログラムがつづきます。”

```python
# ---
def convert_to_conversation(arr_fileimage_DST, arr_fileimage_RAW, sample):
    # ----
    str_dir = sample["dir"]
    str_position = sample["position"]
    str_type = sample["type"]
    str_bbox = sample["bbox_str"]
    # ----
    instruction = "NA"
    if str_type == 'CYLN_DST':
        instruction = instruction_DST
    else:
        instruction = instruction_RAW
    # ----
    index_bbox = str_bbox.index("<loc")
    str_defect = str_bbox[:index_bbox]
    str_defective = "passed"
    if str_defect != "passed":
        str_defective = "defective"
    # ----
    str_answer = f"- image type->{str_type}\n- number of pins->27 pcs\n- your judgement->{str_defective}\n- defect mode->{str_defect}\n- defect position->{str_position}"
    # ----
    str_dir = "drive/MyDrive/passed_image/"
    if str_type == 'CYLN_DST':
        filename = random.choice(arr_fileimage_DST)
        image1 = Image.open(str_dir + filename) 
    else:
        filename = random.choice(arr_fileimage_RAW)
        image1 = Image.open(str_dir + filename).convert("RGB")
    image2 = sample["image"]
    
    # ----
    conversation = [
        { "role": "user",
          "content" : [
            {"type" : "text",  "text"  : instruction},
            {"type" : "image", "image" : image1},
            {"type" : "image", "image" : image2} ]
        },
        { "role" : "assistant",
          "content" : [
            {"type" : "text",  "text"  : str_answer} ]
        },
    ]
    return { "messages" : conversation }

# ---
converted_dataset = [convert_to_conversation(arr_fileimage_DST, arr_fileimage_RAW, sample) for sample in dataset]

# ---
###############################
# SFT TRAINING
###############################
# ---
from unsloth import is_bf16_supported
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig

FastVisionModel.for_training(model) # Enable for training!

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!
    train_dataset = converted_dataset,
    args = SFTConfig(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 20,
        # num_train_epochs = 1, # Set this instead of max_steps for full training runs
        learning_rate = 1.5e-4,
        fp16 = not is_bf16_supported(),
        bf16 = is_bf16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "drive/MyDrive/Unsloth_VLM_1125",
        report_to = "none",     # For Weights and Biases

        # You MUST put the below items for vision finetuning:
        remove_unused_columns = False,
        dataset_text_field = "",
        dataset_kwargs = {"skip_prepare_dataset": True},
        dataset_num_proc = 4,
        max_seq_length = 2048,
    ),
)

# ---
# TRAINING
trainer_stats = trainer.train()

```

D先生 ： “学習が始まりましたね。たった20エポックしか学習しなかったんですか？”

![imageSMNW3-2-3](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-3.jpg)

QEU:FOUNDER ： “今のところは、試験段階なので、ある程度わかればいいのよ。時間もカネ（T4）もかかるし・・・。それにしても、こんなに順調に損失が減るんだったら、30epochはやってみたいですね。”

C部長： “推論結果を見たいですね。”

QEU:FOUNDER ： “それでは、ふたたびプログラムをドン！！基本的には、コードは序盤のものと同じです。”

```python
# ---
###############################
# トレーニング後に推論テストを行う
###############################
# ---
# データ番号(icount)を指定すること
icount = 0
sample = dataset_test[icount]
image1, filename = read_imagefile(arr_fileimage_DST, arr_fileimage_RAW, da-taset_test[icount]["type"])
image2 = dataset_test[icount]["image"]
# ---
message, instruction, str_answer = convert_to_message(sample)
# ---
#print("--- IMAGE(1) ---")
# 画像をarrayに変換
img_measure1 = np.asarray(image1)
str_title1 = f"Image1 - {filename}"
# ---
#print("--- IMAGE(2) ---")
# 画像をarrayに変換
img_measure2 = np.asarray(image2)
str_title2 = f"Image2 - DATA NO:{icount}"
# ---
draw_two_pic(str_title1, img_measure1, str_title2, img_measure2)

# ---
FastVisionModel.for_inference(model) # Enable for inference!

# ---
images = [image1, image2]
input_text = tokenizer.apply_chat_template(message, add_generation_prompt = True)
inputs = tokenizer(
    images,
    input_text,
    add_special_tokens = False,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)
response = tokenizer.batch_decode(outputs)
# ---
split_string = '<|start_header_id|>assistant<|end_header_id|>\n\n'
index_response = response[0].index(split_string)
cut_response = response[0][index_response+len(split_string):]
cut_response = cut_response.replace("<|eot_id|>","")
#print(cut_response)
print("--- INSTRUCTION ---")
display(Markdown(f"<b>{instruction}</b>"))
print("--- AI RESPONSE ---")
display(Markdown(f"<b>{cut_response}</b>"))
print("--- DATASET ANSWER ---")
display(Markdown(f"<b>{str_answer}</b>"))
```

D先生 ： “ワクワク・・・。ドキドキ・・・。”

**（TEST DATASET: i=0）**

![imageSMNW3-2-4](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-4.jpg)

**（TEST DATASET: i=4）**

![imageSMNW3-2-5](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-5.jpg)

D先生 ： “**Defect Modeの予測がうまく行っています**ね。これは驚きました。しかし、残念なのは、**欠陥ピンの位置**ですね。”

![imageSMNW3-2-6](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-6.jpg)

QEU:FOUNDER ： “Image1に英数字を貼っていないからかなあ・・・。ちなみに、image1は画像タイプ毎に6種類準備し、ランダムに呼び出しています。”

D先生 ： “それもあるかもしれません。あと、プロンプト(instruction)にはImage2の英数字のアドレスについての記述がないですよね。”

QEU:FOUNDER ： “さっきも言いましたが、プロンプトを冗長になるのを避けたんです。”

D先生 ： “改善の余地がありますね。”

QEU:FOUNDER ： “つまり、VLMモデルを変える前に、まだまだやることがあります。次回につづきます。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “1年前に言われたじゃないですか、**「東のジャ〇ーズ、西の宝〇」**って・・・。J国って、ハラスメントなくして、組織とコミュニティが持たないようにできているんです。その一方で、ハラスメントが比較的少ない他国の生産性が上がっている。それはなぜか？”

![MOVIE1](http://img.youtube.com/vi/s85CURADbK4/0.jpg)](http://www.youtube.com/watch?v=s85CURADbK4 "ハラスメント理論から見たジャニーズと宝塚。人生と社会の苦悩の本質。安冨歩東大教授。")

C部長 : “**ハラスメントが人々の創造性を殺している**んでしょうね。つまり、J国ではモノとサービスの付加価値を上げる活動が妨害されている。”

![imageSMNW3-2-7](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-7.jpg)

QEU:FOUNDER ： “そんな無駄なことはやめましょう！これがQEUシステムの提案なわけです。思えば、QEUのプロジェクトが本格化した2012？または2013年だったか・・・。ある新聞の1月1日特別版の記事に、**「industrie 4.0にJ国も乗り出す！」**とかいう景気の良い記事が載っていました。その記事には、ある経営者のコメントが載っており、**「この程度のことは、わが社ではすでにやっているが・・・」**と記されていました。おいおい・・・。その「わが社ではすでにやっているが・・・」の維持のために、どれだけの人間が残業をし、パワハラに苦しんでいると思っているんだ？そこら辺に思いを馳せる創造力はないのか？近年のJ国の衰退の根本問題は、ここらへんの想像力の欠如にあると思うんです。何はともあれ、検査から変えていきましょう。検査は、ハラスメントの宝庫だから・・・。”

![imageSMNW3-2-8](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-8.jpg)

QEU:FOUNDER ： “J国の失われた30年・・・。個人的な意見だが、**「すべての前提から間違っていた」ことが不幸の始まり**なのですよ。そして、その影響はいまでもつづいているんです。非常に悪しき形で・・・。J国は、歴史的に別にすごかったことはないし、NO１でもなかった・・・。ず～っとね・・・。”

![imageSMNW3-2-9](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-9.jpg)

C部長 : “WALK MANを作った会社なんか、すごかったじゃないですか？”

QEU:FOUNDER ： “O谷選手はすごいけど、別にJ国がすごいわけじゃない。”

C部長 : “・・・。おじいさんたち、**思いがけず白人様におだてられて舞い上がっていた**んじゃないですか？そして、**その幻想が「J国すごい」として、いまでも残っている**と・・・。J国がすごかったのは、人口ボーナス（若年労働者が多かった）があったころだけです。そのボーナスが終わったときに、**考え方を根本的に変えるべき**でした。”

![MOVIE2](http://img.youtube.com/vi/CPvPmdQRDzo/0.jpg)](http://www.youtube.com/watch?v=CPvPmdQRDzo "時代が進むほど日本が息苦しくなる理由。日本衰退の原因に気付いて、あなたの人生だけでも救おう！メールボーイ、電話交換手、FAX、そして電子メール。安冨歩東大教授。")

QEU:FOUNDER ： “**右の人は、何事も問題を「～の徹底」という極端化で解決しようとする**でしょ？彼らが生産性を上げようとすると、こうなる（↓）・・・。”

![imageSMNW3-2-10](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-10.jpg)

C部長 : “そして、ドツボ(↓)にはまって今に至る・・・。”

![imageSMNW3-2-11](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-11.jpg)

QEU:FOUNDER ： “ただし、いわゆる「右」といってもねえ・・・。実際のところ、政治的には間口が広いんですよ。ホレ・・・、**「れ」のつく党（↓）**を見てみ・・・。”

![imageSMNW3-2-12](/2024-11-26-QEUR23_SMNW11/imageSMNW3-2-12.jpg)

C部長 : “あらあら・・・。半分以上が・・・。”

QEU:FOUNDER ： “あそこが「いまいち伸びない」のは、こういう部分なんでしょうね。”
