---
title: Technology Review: Summary - Automated Visual Inspection Machines Using SOARTC Metrics
date: 2024-09-18
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "Vision language Model"]
excerpt: Vision Transformer(ViT)をやってみる
---

### Name of invention 

Visual inspection automatic machine using SOARTC metrics

### Technology field

The present invention relates to an automatic computer-based visual inspection machine.

### Technology background

Machine learning logic for image classification has already been proposed, and many results have been achieved (Fig. 1). Especially, CNN (Convolutional Neural Network) is considered to have high discrimination ability.

**(Fig. 1: History of image recognition competitions)**
![imageVLM1-2-1](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-1.jpg)

However, if these techniques that have been successful in image discrimination (Fig. 2) are used as is to detect abnormalities through visual inspection, good results may not necessarily be obtained. On the other hand, a technology called Vision Transformer (ViT) has recently been proposed, and this method may be better than CNN for visual inspection (anomaly detection). CNN aggregates image information through convolution to create feature vectors, making it excellent for individual discrimination (Fig. 3). However, due to the characteristics of the model structure (Fig. 4), CNN does not have good discrimination accuracy when the features of the test object are spatially separated. On the other hand, since ViT is a technology derived from natural language processing, there are no such limitations. In some cases, academic results that applied ViT to quality control have been published as papers (Fig. 5).

**(Fig. 2: Examples of Image Recognition)**

![imageVLM1-2-2](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-2.jpg)

**(Fig. 3: Structure of convolutional neural network)**

![imageVLM1-2-3](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-3.jpg)

**(Fig. 4: Advantage of Vision Transformer)**

![imageVLM1-2-4](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-4.jpg)

**(Fig. 5: Examples of academic achievements)**

![imageVLM1-2-5](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-5.jpg)

In addition to the above-mentioned CNN and ViT, other anomaly detection methods have been proposed, and have been used for products with extremely high transferability, such as for quality control of printed circuit boards. Typical methods are listed below.

- Image subtraction method
- How to measure distance
- Supervised learning (deep learning)
- *Autoencoder (VAE) is considered an intermediate method between deep learning and image difference method.

"Image difference method" and "method for measuring distance" may have limited applicability. On the other hand, it is considered that individual discrimination technology such as CNN or ViT (Vision Transformer) applied for abnormality detection will be better. Especially, the method of creating a composite image using SOART3 metrics for ViT to detect abnormalities, which this inventor proposed in a previous paper, is considered to be promising for visual inspection of objects with complex shapes (Fig. 6). 

**(Fig. 6: Examples of inspected products - plastic injection molded products)**

![imageVLM1-2-6](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-6.jpg)

In the previously reported invention (hereinafter referred to as the "previous invention"), even minute defects that can only be detected using high-resolution (ex 1000x1000) images can be reduced to standard size (224x224) ViT models use. Therefore, compared to conventional methods, the previous invention can make highly accurate predictions without incurring training data preparation costs and calculation costs. Since the present invention also uses some of the know-how from the previous invention, the technology will be briefly explained below.

RT method of the Taguchi method uses Eugridian distance, which is a comparison of the standard vector and the measured vector after correction by sensitivity (β), as the signal-to-noise ratio (η) (Fig. 7). On the other hand, SOART3 method extends RT method to three-dimensional metrics output (Fig. 8) and outputs them as RGB images. Figure 8 shows the system configuration.

**(Fig. 7: Principle of RT method)**

![imageVLM1-2-7](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-7.jpg)

**(Fig. 8: System configuration diagram of SOART3 method)**

![imageVLM1-2-8](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-8.jpg)

SOART3 method universalizes the distance between the standard vector and the measured vector and considers the Minkowski distance (Fig. 9). This distance can be changed from Manhattan distance (p=0) to Chebyshev distance (p=∞) by changing the p value of the parameter. In other words, SOART3 method uses the Manhattan distance and Chebyshev distance instead of the Eugridian distance of RT method because we want to separate them into two independent metrics and make them three-dimensional (Fig. 10).

**(Fig. 9: Definition of Minkowski distance)**

![imageVLM1-2-9](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-9.jpg)

**(Fig. 10: Concept of SOART3 method)**

![imageVLM1-2-10](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-10.jpg)

Here is an example of SOART3 method described in a Python program. First, calculate the sensitivity (β) from the standard vector (x) and measurement vector (y) data. Then, calculate the Chebyshev distance and Manhattan distance between the sensitivity-corrected βx and the measurement vector y, respectively. The Chebyshev distance and the Manhattan distance are generated from the same vector (βx, y), and there is a correlation between the two distances, so the Chebyshev distance is logarithmically transformed and then the difference from the Manhattan distance is taken. Logarithmic transformation is applied to linearize the value behavior and reduce the size of the learning model.

```python

# soaRT3メトリックスを計算する
def calc_soaRT3(tsr_sig_array, tsr_tani_array): 

    # データの抽出
    y = tsr_sig_array
    x = tsr_tani_array
    #print(y)
    #print(x)

    # 感度(β)を計測
    xx = np.dot(x,x) + 0.0001
    xy = np.dot(x,y) + 0.0001
    beta = xy/xx

    # チェビシェフ距離を計測
    vDistance = chebyshev(y,beta*x)

    # マンハッタン距離を計測
    mDistance = np.linalg.norm(y - beta*x, ord=1)
    #print("mDistance: ", mDistance.item())
    
    # 値の対数変換
    log_beta  = math.log(beta)
    log_yita = math.log(mDistance+1.0)
    log_gamma = math.log(vDistance+1.0) - log_yita
    
    return log_beta, log_yita, log_gamma

```

SOART3 metrics are obtained by processing standard images (standard vectors) and measured images (measurement vectors). By arranging the metrics pixel by pixel, we can generate a SOART3 composite image like the one below (Fig. 11).

**(Fig. 11: SOART3 composite image example)**

![imageVLM1-2-11](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-11.jpg)

As an example of the application of the previous invention to the terminal disconnection inspection of the connector, we will introduce the results of an experiment using learning and verification data of normal, defective with tilted mode, and defective with backward mode (Fig. 12).

**(Fig. 12: Mahalanobis distance)**

![imageVLM1-2-12](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-12.jpg)

Using the previously invented ViT, it is not only possible to classify (or detect) abnormal modes, but also to visualize abnormal locations using the Attention Map function (Fig. 13).

**(Fig. 13: Attention map example)**

![imageVLM1-2-13](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-13.jpg)

### Problems to be solved by this invention.

Considering the practice of visual inspection, even the aforementioned 1000 x 1000 resolution may not be sufficient for the input image. Depending on the type of defect to be detected, a high-resolution input of 4000x4000 or more is required (Fig. 14). Of course, there is also a method of dividing the area to be inspected and predicting it individually. However, in that case, the cost will be higher because of multiple ViT engines.

**(Fig. 14: Example of defect that requires high-resolution)**

![imageVLM1-2-14](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-14.jpg)

Furthermore, in order to reduce the cost of automatic visual inspection machines, it is necessary to be able to predict and visualize them without using complex ViT. Furthermore, it would be desirable to have simpler prediction methods that could be tested on low-cost edge computers such as  Raspberry Pi (Fig. 15).

**(Fig. 15: Raspberry Pi5 specs)**

![imageVLM1-2-15](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-15.jpg)

### Means to solve problems.

In this invention, we extend SOART3 method and ViT (Vision Transformer) concept of the previous invention to generate a composite image and visualize the visual inspection results. The present invention (hereinafter referred to as SOARTC method) consists of two analysis steps. Among them, STEP 1 convolution is added to SOART3 scheme (Fig. 16). Then, STEP 2 is the measurement of the degree of abnormality (Fig. 17), which corresponds to ViT of the previous invention.

**(Fig. 16: Structure of SOARTC metrics – STEP1)**

![imageVLM1-2-16](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-16.jpg)

**(Fig. 17: Structure of SOARTC metrics – STEP2)**

![imageVLM1-2-17](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-17.jpg)

SOART3 method inputs two images: a standard image and a measurement image. On the other hand, in the first step (STEP 1) of SOARTC method, the input image is convolved to generate self-similarity metrics. Users must design the parts for performing the convolution themselves. Whereas DATUM is the part for generating standard matrices, and other parts are designed for measurement matrices. Figure 18 shows an example of a 5x5 convolution design, but it can be made to other sizes (such as 11x11).

**(Fig. 18: convolution parts)**

![imageVLM1-2-18](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-18.jpg)

Images input into the visual inspection system are cropped to a predetermined size to generate SOARTC metrics. In the example in Figure 19, it is cut to size 22x22. This is because the size of the convolution part is 11x11 and the convolution produces a 2x2 self-similarity matrix.

**(Fig. 19: Design of self-similarity matrix - 2x2 case)**

![imageVLM1-2-19](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-19.jpg)

The self-similarity matrix generated in this way is processed by SOART3 method and converted into three metrics (β, η, γ). As a result, the output of the STEP1 will be 3x6 (convolution components) = 18 types of vectors as shown in Figure 20. This method is similar to attention in that it uses self-similarity (Fig. 21).

**(Fig. 20: SOARTC output of STEP1)**

![imageVLM1-2-20](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-20.jpg)

**(Fig. 21: Structure of attention scheme)**

![imageVLM1-2-21](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-21.jpg)

The STEP2 uses the output of the first stage as input. Below, we will explain the second stage method while looking at examples of various shapes analyzed using SOARTC. Whereas the standard image is the square with rounded corners shown in the red frame, and the other shapes are the measurement images.

**(Fig. 22: Test images and their shapes)**

![imageVLM1-2-22](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-22.jpg)

In the STEP2, the 3x6=18 types of metrics output in STEP1 are classified into types of beta(β)-yita(η)-gamma(γ), and the metrics values (X) of the standard image are Compare the metrics values (Y) of the measurement images using a scatter plot. Then, the degree of deviation from the 0-point linear equation of β (slope) = 1 becomes the degree of abnormality.

**(Fig. 23: Results of drawing a scatter plot using SOART3 three-type metrics)**

![imageVLM1-2-23](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-23.jpg) 
	 
Quantifies the degree to which these plots deviate from the zero-point linear equation with slope β = 1. The method of quantification can be selected depending on the user's way of thinking. Whereas we used the Minkowski distance between the standard vector and the measured vector.

```python

# ----------------
# soaRTCメトリックス(no2)を計算する
def calc_soaRTC_no2(tsr_sig_array, tsr_tani_array): 

    # データの抽出
    y = tsr_sig_array
    x = tsr_tani_array
    #print(y)
    #print(x)

    # dot回転を計測
    #xx = np.dot(x,x) + 0.0001
    #xy = np.dot(x,y) + 0.0001
    #beta = xy/xx

    # チェビシェフ距離を計測
    #vDistance = chebyshev(y,beta*x)
    #vDistance = chebyshev(y,x)
 
    # ミンコフスキー距離を計測
    vDistance = minkowski(y, x, 4) #p=4 
 
    return vDistance

```

Therefore, SOARTC method makes it very easy to increase the size of the input image. You can make the convolution parts larger, and you can also make the stride amount larger. Furthermore, the 2x2 self-similarity matrix can be made larger to 4x4.

**(Fig. 24: Mechanism of stereoscopic measurement using double-eyes method)**

![imageVLM1-2-24](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-24.jpg)

Using SOARTC method, it is easy to combine the inspection images from the left and right cameras and inspect the quality of three-dimensional products.

### Example 1

In this example, we will create various square images as shown in Figure 25 and measure SOARTC metrics values. Elements such as size, color, rotation, and movement (X, Y) are added to the image, but we also prepared a similar image for a triangle as a contrast of shapes.

**(Fig. 25: Image used in the experiment - square case)**

![imageVLM1-2-25](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-25.jpg)

**(Fig. 26: List of prepared test images)**

![imageVLM1-2-26](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-26.jpg)

The relationship between rotation angle and SOARTC metrics was evaluated graphically. The three graphs in Figure 27 each represent the relationship between rotation angle and beta-yita-gamma metrics, and each graph compares squares and triangles. By looking at this, you can evaluate how shape differences, rotations, and shift affect metrics.

**(Fig. 27: Table of three types of metrics calculated by SOARTC method)**

![imageVLM1-2-27](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-27.jpg)

**(Fig. 28: Graphing SOARTC metrics - rotation and shift)**

![imageVLM1-2-28](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-28.jpg)

The knowledge obtained here is that when SOARTC method is used for visual inspection, even if a slight deviation (rotation, movement) occurs when the object to be inspected is installed, it can sufficiently detect abnormalities in the image's shape. It's possible. Next, evaluate the influence of the size and color (brightness) of the figure in the same way.

**(Fig. 29: Graphing SOARTC metrics - size and color)**

![imageVLM1-2-29](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-29.jpg)

On the other hand, regarding the relationship between changes in shape size and color and metrics, we found that the amounts of these changes are close to the differences in shapes. In other words, when creating a visual inspection machine using SOARTC method, you need to pay attention to the stability of the size (distance between the camera and the subject) and brightness (distance between the light and the subject). Conversely, we found that the SOART method can easily detect abnormalities in size and color.

### Example 2

As the next example, we performed a simulation of the use of SOARTC method in an environment closer to actual visual inspection. We inspected the wire harness connector for loose terminals. The images used here were created using three-dimensional CG software (Blender). In this way, by creating images using CG software (Fig. 30), it is possible to generate a large number of images with subtle differences in the rotation and movement of the inspected object (Fig. 31).

**(Fig. 30: Prismatic CG model)**

![imageVLM1-2-30](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-30.jpg)

**(Fig. 31: Image group of prismatic connectors)**

![imageVLM1-2-31](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-31.jpg)

As standard images used in SOARTC method, we prepared multiple images of connectors in normal condition and averaged them. Additionally, as the image for measurement (external appearance inspection), we used an image with the upper left pin tilted.

**(Fig. 32: standard image)**

![imageVLM1-2-32](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-32.jpg)

**(Fig. 33: Measurement image)**

![imageVLM1-2-33](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-33.jpg)

First, let's look at RGB images using SOARTC metrics. SOARTC metrics provide three types of numerical values: beta, yita, and gamma, and these values are converted into a color image. This image allows you to see the difference between the standard image and the measured image.

**(Fig. 34: SOARTC analysis results ~ RGB image)**

![imageVLM1-2-34](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-34.jpg)

Figure 35 shows the result of converting this RGB image to grayscale. Whereas it is best to decide the conversion formula for converting an RGB image to grayscale by yourself (Gray=αxRed+βxGreen+γxBlue) to make it easier for the user to discover abnormalities.

**(Fig. 35: SOARTC analysis results ~ Grayscale image)**

![imageVLM1-2-35](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-35.jpg)

In order to make abnormal areas more noticeable, the image shown in Figure 36 is obtained by processing the grayscale image with the softmax function.

**(Fig. 36: SOARTC analysis results ~ after softmax function processing)**

![imageVLM1-2-36](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-36.jpg)

For an example, that is easier to understand, we processed an image of a normal inspected product with text pasted on top using SOARTC method (Fig. 37). You can now see that the characters are highlighted in the processed image.

**(Fig. 37: When text is pasted on an image of a normal inspected product)**

![imageVLM1-2-37](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-37.jpg)

**(Fig. 38: SOARTC analysis results)**

![imageVLM1-2-38](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-38.jpg)

Thus, you can obtain an image highlighting abnormal areas that corresponds to the attention map that can be output by ViT by using SOARTC method. However, in the case of SOARTC, since the image shows the difference between the standard image and the measured image, it also displays the tilt and rotation of the image other than the abnormality targeted by the inspection.

### Industrial applicability

In this invention, we proposed SOARTC metrics as a feature engineering method for analyzing large images with small ViT models. At the same time, the output images can also be used to assist in visual inspections as equivalent to attention maps.

**(Fig. 39: Application example for missing item inspection)**

![imageVLM1-2-39](/2024-09-18-QEUR23_VIVLM1/imageVLM1-2-39.jpg)

Since the present invention makes it possible to compare and visualize the differences between standard images and measured images, it will be possible to automate visual inspections by inputting the output images into a relatively simple model such as an SVM (support vector machine). In particular, it can be easily applied to relatively simple automation projects such as out-of-stock inspections.

