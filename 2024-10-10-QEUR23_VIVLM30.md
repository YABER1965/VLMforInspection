---
title: QEUR23_VIVLM30:  Unslothでやってみた(他のインスタンスでやってみた)
date: 2024-10-10
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "Vision language Model"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VIVLM30:  Unslothでやってみた(他のインスタンスでやってみた)

## ～ 他のやり方もやってみましょう ～

QEU:FOUNDER ： “今回から、いよいよUnslothだ！！”

![imageVLM1-30-1](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-1.jpg)

D先生 ： “よっしゃぁ・・・！！”

QEU:FOUNDER ： “せめて気持ちだけでも前向きにして、新プロジェクトを始めましょう（笑）。ドン！！”

```python
# ---
!pip install unsloth
!pip install -U Requests torchvision transformers accelerate peft datasets wandb
# Also get the latest nightly Unsloth!
!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

```

QEU:FOUNDER ： “インストールするパッケージ群（↑）は余分に指定しています。これは、そもそも、学習用の設定です。ただし、今回は推論のみです。”

```python
# ---
import requests
import torch
from unsloth import FastLanguageModel
from PIL import Image
from transformers import AutoProcessor, MllamaForConditionalGeneration, MllamaProcessor
import matplotlib.pyplot as plt  # Importing Matplotlib

# ---
torch.manual_seed(3)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# ---
# unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit
# unsloth/Llama-3.2-3B-bnb-4bit
# unsloth/Meta-Llama-3.1-8B
# unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit
# ---
model_id = " unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit"
torch_dtype=torch.bfloat16)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_id,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)
FastLanguageModel.for_inference(model) # Enable native 2x faster inference

```

QEU:FOUNDER ： “どうも、FastLanguageModelクラスはVISIONモデルをサポートしていないらしい。早めに**アップグレード**すればいいのだが・・・。”

![imageVLM1-30-2](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-2.jpg)

D先生 ： “あらあら・・・。そういえば、Communityによれば、作者からコメントが出ていましたね。”

![imageVLM1-30-3](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-3.jpg)

QEU:FOUNDER ： “ただし、このモデルをつかって、むりやり推論をやれないことはありません。**これから「プログラムの一部を改造」します**。ドン！！”

```python
# ---
# Load local image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# ----
# 読み込んだ画像の幅、高さを取得し半分に
(width, height) = (image.width // 2, image.height // 2)
# 画像をリサイズする
img_resized = image.resize((width, height))

# ----
# Convert image to array for Matplotlib
image_array = img_resized.convert("RGB")

# Display the image using Matplotlib
plt.imshow(image_array)
plt.axis('off')  # Hide axes
plt.show()  # Show the image

```

QEU:FOUNDER ： “最初はネタの準備です。まずはイメージをみてみましょう。”

![imageVLM1-30-4](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-4.jpg)

D先生 ： “良いイラストですね。この画像を使って、モデルに内容を語ってもらうんですね。”

QEU:FOUNDER ： “以下のコードは、普通のLlama3.2-vision用のクラスを転用したものです。”

```python
# ---
# このモデル読み込みは伝統的なもの
model = MllamaForConditionalGeneration.from_pretrained(model_id, device_map="auto", torch_dtype=torch.bfloat16)
processor = MllamaProcessor.from_pretrained(model_id)
# 以下はメッセージの提供
messages = [
    [
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": "What does the image show?"}
            ]
        }
    ],
]
text = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=text, images=img_resized, return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=25)
print(processor.decode(output[0]))

```

D先生 ： “へえ・・・、こんなやり方でもやれるんですね。”

![imageVLM1-30-5](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-5.jpg)

QEU:FOUNDER ： “つまり、モデル自体には問題はありません。しかし、**ツールとなっているクラス・インスタンスが機能していません**。”

D先生 ： “ああ、残念・・・。トレーニング（ファインチューニング）はできるんですかね？”

![imageVLM1-30-6](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-6.jpg)

QEU:FOUNDER ： “学習のためのコレータが全然かわると思うので、今の時点では、我々は手を出せないですね。さてと・・・。思いがけずに時間が空いたので、代わりにコレ（↑）をやってみましょうか？”

![imageVLM1-30-7](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-7.jpg)

D先生 ： “これ、人気があるそうですね。画像にキャプションを付けることができるのがうれしいです。”

QEU:FOUNDER ： “ほんとうは、このモデルもUnslothしてもらいたいね・・・(笑)。”


## ～ まとめ ～

QEU:FOUNDER ： “なるほど！・・・と、思いました。いかに売れっ子であったライターであっても、**「x0歳」を過ぎれば仕事がなくなる**のですね。”

[![MOVIE1](http://img.youtube.com/vi/pjecfcOjGio/0.jpg)](http://www.youtube.com/watch?v=pjecfcOjGio "（その３）復活！水道橋博士×町山智浩 唐沢俊一と雑誌時代の『SHI』")

C部長 : “考えてみれば、当たり前です。 “

QEU:FOUNDER ： “このように考えれば、この雑誌（↓）の見方がかわるでしょ？”

![imageVLM1-30-8](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-8.jpg)

C部長 : “ありがたい雑誌です。しかし、**「高齢者が活躍する社会の闇」**を見たような気がします。 “

**（好き。エズさん）**

![imageVLM1-30-9](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-9.jpg)

**（好き好き。エズさん）**

![imageVLM1-30-10](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-10.jpg)

**（好き好き大好き。エズさん）**

![imageVLM1-30-11](/2024-10-10-QEUR23_VIVLM30/imageVLM1-30-11.jpg)

QEU:FOUNDER ： “**高齢者がリタイア後に人生をエンジョイできる社会を作る方が、社会の損失がずっと少なくなると思います**。”
