---
title: QEUR23_VIVLM34 – Loraで簡単なデータセットを学習する
date: 2024-10-20
tags: ["QEUシステム", "メトリックス", "Python言語", "Vision Transformer", "LLM", "データセット", "Fine-tuning", "Vision language Model"]
excerpt: Vision Transformer(ViT)をやってみる
---

## QEUR23_VIVLM34 – Loraで簡単なデータセットを学習する

## ～ すごく簡単なデータセットを使って学習をしましょう ～

QEU:FOUNDER ： “もう一度、ちょっと趣を変えて**Florence-2モデルのファイン・チューニング**をしましょう。”

![imageVLM1-34-1](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-1.jpg)

QEU:FOUNDER ： “前回のファイン・チューニング（↓）のトライアルでは、モデルの半分のパラメタが書き換えられました。さすがに、そこまでやるのはうれしくないです。”

**（参考：前回のファイン・チューニング）**

![imageVLM1-34-2](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-2.jpg)

D先生 ： “前回から、計算負荷に関して、FOUNDERは不満でした。さて、その後に、いいシーズが見つかりましたか？￥”

![imageVLM1-34-3](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-3.jpg)

QEU:FOUNDER ： “いくつか参考になるコードが見つかったので、それらを組み合わせて**PEFT（LORA）版のファイン・チューニング**をやってみましょう。今回は、先ほど挙げたTU語のキャプションがついた簡易版のデータセットを使います。”

```python
##########################################
# READ DATASET
##########################################
# ---
import re
from PIL import Image
from datasets import load_dataset, DatasetDict, Dataset

# ---
# データセットの読み込み
dataset = load_dataset("YxBxRyXJx/cut_TRV_ver2_1019")
split_dataset = dataset["train"].train_test_split(test_size=0.2, shuffle=True)
ds_train = split_dataset["train"]
ds_val = split_dataset["test"]
ds_test = dataset["test"]
print("Train len: ", len(ds_train))
print("Val len: ", len(ds_val))
print("Test len: ", len(ds_test))

# ---
import supervision as sv

# ---
# データ抽出
def extract_loc_values_and_labels(bbox_str):
    print(bbox_str)
    bbox_label_pairs = re.findall(r'([\w\s]+)(<loc_\d+><loc_\d+><loc_\d+><loc_\d+>)', bbox_str)
    print("bbox_label_pairs:",bbox_label_pairs)
    # ---
    bboxes = []
    labels = []

    for label, bbox in bbox_label_pairs:
        loc_values = re.findall(r'<loc_(\d+)>', bbox)
        loc_values = [int(x) for x in loc_values]
        # convert to PASCAL VOC format
        x1, y1, x2, y2 = loc_values
        print("ORIGINAL - x1:{}, y1:{}, x2:{}, y2:{}".format(x1, y1, x2, y2))
        # ---
        loc_values = [
            int(loc_values[0]), int(loc_values[1]),
            int(loc_values[2]), int(loc_values[3]),
        ]
        bboxes.append(loc_values)
        labels.append(label)

    return bboxes, labels

# ----
# フォーマット形成
def create_response(bboxes, labels):

    part_response = {'bboxes': bboxes, 'labels': labels}
    response = {'<OD>': part_response}

    return response

# ---
# SUPERVISINGで画像を描く
def draw_DBimage(img_resize_lanczos, response):
    detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolu-tion_wh=img_resize_lanczos.size)
    bounding_box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)
    label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)
    # ---
    image_new = bounding_box_annotator.annotate(img_resize_lanczos, detections)
    image_new = label_annotator.annotate(image_new, detections)
    #image_new.thumbnail((600, 600))
    return image_new

# ----
bbox_str = ds_test[0]["suffix"]
image = ds_test[0]["image"]
print(bbox_str)
print(image.width, image.height)
# ----
bboxes, labels = extract_loc_values_and_labels(bbox_str)
print("bboxes: ", bboxes)
print("labels: ", labels)

# ---
response = create_response(bboxes, labels)
print(response)


##############################
# captionをつけた画像を描く
##############################
# ---
image_new = draw_DBimage(image, response)
image_new

```

D先生 ： “これは、データセットの画像からですね。”

![imageVLM1-34-4](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-4.jpg)

D先生 ： “おっと、ちゃんとTU語になっているじゃないですか・・・。もうモデルが学習したのかな？”

QEU:FOUNDER ： “わけない！ここまでのコードをよく見てください。データセットのキャプションをそのままに読み込んでいるだけです。今回から、キャプション付き画像を生成するためにsuperviseというモジュールを使用します。いままでのキャプションはmatplotlib上での上書きだったので、画像とキャプションは一体化されていませんでした。今回は、よりよい仕上げになっているでしょ？さて、お次は、モデルを読み込んで推論してみましょう。”

```python
################################3
# LOADING MODEL
#################################
# ---
# Imports
import json
import torch
import html
import base64
import itertools
import numpy as np

# ---
from IPython.core.display import display, HTML
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AdamW,
    AutoModelForCausalLM,
    AutoProcessor,
    get_scheduler
)
from tqdm import tqdm
from typing import List, Dict, Any, Tuple, Generator
from peft import LoraConfig, get_peft_model
from PIL import Image

# ---
CHECKPOINT = "microsoft/Florence-2-base-ft"
REVISION = 'refs/pr/6'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True, revi-sion=REVISION).to(DEVICE)
processor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True, revi-sion=REVISION)

# ---
# 画像の読み込み
image = ds_test[3]["image"]

# ---
task = "<OD>"
text = "<OD>"

inputs = processor(text=text, images=image, return_tensors="pt").to(DEVICE)
generated_ids = model.generate(
    input_ids=inputs["input_ids"],
    pixel_values=inputs["pixel_values"],
    max_new_tokens=512,
    num_beams=3
)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))
print(response)

# ---
# SUPERVISINGで画像を描く
def draw_DBimage(image, response):
    detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolu-tion_wh=image.size)
    bounding_box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)
    label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)
    # ---
    image_new = bounding_box_annotator.annotate(image, detections)
    image_new = label_annotator.annotate(image_new, detections)
    #image_new.thumbnail((600, 600))
    return image_new

image_new = draw_DBimage(image, response)
image_new

```

QEU:FOUNDER ： “物体検出(OD)の推論の結果を見てみましょう。これは、Pre-trainモデルを、そのまま使用したものです。”

![imageVLM1-34-5](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-5.jpg)

D先生 ： “さすがにPre-trainモデルはうまく検出してくれますね。”

QEU:FOUNDER ： “ここからがモデルの学習準備になります。読み込んだTRVデータセットを加工して、モデルが学習のために消化しやすい状態に加工します。”

```python
# ---
# DBフォーマットを整形する
class DOTRADataset(Dataset): 

    def __init__(self, data): 
        self.data = data
        
    def __len__(self): 
        return len(self.data)
        
    def __getitem__(self, idx):
        example = self.data[idx]
        question = "<OD>"
        answer = example['suffix']
        #image = example['image'].convert("RGB")
        image = example['image']
        return question, answer, image    
    
# ---
# データセットを生成する
def collate_fn(batch):
    questions, answers, images = zip(*batch)
    inputs = processor(text=list(questions), images=list(images), return_tensors="pt", pad-ding=True).to(DEVICE)
    return inputs, answers

# ---
train_dataset = DOTRADataset(ds_train)
val_dataset = DOTRADataset(ds_val) 

# ---
batch_size = 6
num_workers = 0

train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                          collate_fn=collate_fn, num_workers=num_workers, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                          collate_fn=collate_fn, num_workers=num_workers)		
		
```

QEU:FOUNDER ： “DataLoaderの処理によって、データセットをモデルに効率的に供給する準備ができました。データベースがテンソル化されています。”

![imageVLM1-34-6](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-6.jpg)

D先生 ： “これから、LoRAによる学習が始まりますね。”

```python
# ---
config = LoraConfig(
    r=8,
    lora_alpha=8,
    target_modules=["q_proj", "o_proj", "k_proj", "v_proj", "linear", "Conv2d", "lm_head", "fc2"],
    task_type="CAUSAL_LM",
    lora_dropout=0.05,
    bias="none",
    inference_mode=False,
    use_rslora=True,
    init_lora_weights="gaussian",
    revision=REVISION
)

peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()
# ---
torch.cuda.empty_cache()

```

QEU:FOUNDER ： “LoRAなので、学習されるパラメタがすくなくなったでしょう？”

![imageVLM1-34-7](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-7.jpg)

D先生 ： “あらら・・・、これはすごい。前回のファイン・チューニングよりも、ずいぶん学習対象パラメタが少なくなりましたね。”

QEU:FOUNDER ： “ここらへんの工夫が「計算料金」に大きく効くんですよ。だから、前回のモデルで続きの展開をやりたくなかったわけで・・・。さらに続けましょう。”

```python
# ---
def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):
    optimizer = AdamW(model.parameters(), lr=lr)
    num_training_steps = epochs * len(train_loader)
    lr_scheduler = get_scheduler(
        name="linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps,
    )

    #render_inference_results(peft_model, ds_test)

    # ---
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for inputs, answers in tqdm(train_loader, desc=f"Training Epoch {epoch + 1}/{epochs}"):

            input_ids = inputs["input_ids"]
            pixel_values = inputs["pixel_values"]
            labels = processor.tokenizer(
                text=answers,
                return_tensors="pt",
                padding=True,
                return_token_type_ids=False
            ).input_ids.to(DEVICE)

            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)
            loss = outputs.loss

            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        print(f"Average Training Loss: {avg_train_loss}")

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for inputs, answers in tqdm(val_loader, desc=f"Validation Epoch {epoch + 1}/{epochs}"):

                input_ids = inputs["input_ids"]
                pixel_values = inputs["pixel_values"]
                labels = processor.tokenizer(
                    text=answers,
                    return_tensors="pt",
                    padding=True,
                    return_token_type_ids=False
                ).input_ids.to(DEVICE)

                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)
                loss = outputs.loss

                val_loss += loss.item()

            avg_val_loss = val_loss / len(val_loader)
            print(f"Average Validation Loss: {avg_val_loss}")

            render_inference_results(peft_model, ds_test)
        # ---
        output_dir = f"drive/MyDrive/FTmodels/model_checkpoints/epoch_{epoch+1}"
        os.makedirs(output_dir, exist_ok=True)
        model.save_pretrained(output_dir)
        processor.save_pretrained(output_dir)

# ---
# Run train loop
%%time
EPOCHS = 2
LR = 5e-6

train_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)

```

QEU:FOUNDER ： “学習の結果を覗いてみましょう。”

![imageVLM1-34-8](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-8.jpg)

D先生 ： “ほう・・・、プログラムは、ちゃんと動いているようです。それにしても、エポック数を、**「たったの２」**にしたんですか？”

![imageVLM1-34-9](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-9.jpg)

QEU:FOUNDER ： “今回は、LoRAトライアルの最初の最初の一歩だからね。なんといっても、逆説的になるが、小生としては書き換えつパラメタ数が0.7％にしかならないのが心配・・・（笑）。あとで、LoraConfig変更を検討してみようかと思います。”

D先生 ： “ダメだとは思いますが、今回の学習済のモデルで推論してみましたか？”

QEU:FOUNDER ： “画像を読み込んで、推論をしてみました。ドン！！”

```python
# ---
EXAMPLE_IMAGE_PATH = "drive/MyDrive/IMG_5968.JPG"
image_org = Image.open(EXAMPLE_IMAGE_PATH)
image = image_org.resize((512, 412))
image

# ---
task = "<OD>"
text = "<OD>"

inputs = processor(text=text, images=image, return_tensors="pt").to(DEVICE)
generated_ids = peft_model.generate(
    input_ids=inputs["input_ids"],
    pixel_values=inputs["pixel_values"],
    max_new_tokens=512,
    num_beams=3
)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))
detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)

bounding_box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)
label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)

image_inf = bounding_box_annotator.annotate(image, detections)
image_inf = label_annotator.annotate(image_inf, detections)
image_inf

```

QEU:FOUNDER ： “おなじみの、かわいい猫の画像です。”

![imageVLM1-34-10](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-10.jpg)

D先生 ： “あれ？キャプションが、E語のままですね。”

QEU:FOUNDER ： “まあ・・・、学習回数はたったの2epochだけだからね。”

D先生 ： “肝心の計算時間は短くなりましたか？”

![imageVLM1-34-11](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-11.jpg)

QEU:FOUNDER ： “計算時間の定義が全く同じかわからないが、2epochという比較では、前回とほぼ同じ計算時間です。ただし、前回はA100のGPUを使っており、今回はL4を使っています。多分、**画像サイズも大きくはないのでT4のGPUでも十分だ**と思います。”

D先生 ： “なるほど。そうなると計算コストが全く違いますね。”

QEU:FOUNDER ： “外観検査データセットの学習では、epochは10位にしようと思います。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “国民民〇党は**ワンダーランド**・・・。しかし、J国の若者はすごいと思うわ・・・。**若い時には、貧乏をエンジョイし、年を取ったら姥捨**なんでしょ？”

![imageVLM1-34-12](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-12.jpg)

C部長 : “美しいJ国の経済学、スゴイ！！心から思うわ・・・。こんな考え方が前提になった社会で、イノベーションが起きて、発展すると思っているのだろうか・・・。この人たち（↑）は・・・。“

![imageVLM1-34-13](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-13.jpg)

QEU:FOUNDER ： “姥捨思想では、社会は発展しないと思います。**人々の意識と社会がシュリンクしていくだけ**です。”

C部長 : “はい。ボクも感覚的には分かります。でも、なんでかなあ・・・。 “

![imageVLM1-34-14](/2024-10-20-QEUR23_VIVLM34/imageVLM1-34-14.jpg)

QEU:FOUNDER ： “**人々が前向きに生きるための、最も障害となる意識は「恥」らしい**よ。J国は空気を使って意図的にターゲットになる人々を**「恥に追い込む」**でしょう？”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

C部長 : “このおっさん（↑）はQCサークル大会の訓示として、がんばって成果を出してくれたQCサークルのメンバーたちに向かって、こんなアホなことを言ったんでしょう？ああ・・・。恥ずかしい・・・。こんな（↑）に**「くだらない管理（ハラスメント）」**のために、無駄に金と時間、気力を使わず。もっと前向き、創造的なことをしたいですね。 “

QEU:FOUNDER ： “このおっさんの訓示が正しいものであるとするならば、**もうQCサークルなんかいらないですよ。永遠にね・・・。**”

