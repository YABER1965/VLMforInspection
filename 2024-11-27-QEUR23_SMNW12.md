---
title: QEUR23_SMNW12 – Unslothで1SHOT推論を試験してみる
date: 2024-11-27
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_SMNW12 – Unslothで1SHOT推論を試験してみる

## ～ 結果は失敗だが、次への布石です ～

D先生 ： “前回は最も基礎的な2画像比較型のプロンプトを使ったファイン・チューニングをやってみました。その結果としては、我々が当初想定したよりも良い出来でした。そうですよね？”

![imageSMNW3-3-1](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-1.jpg)

QEU:FOUNDER ： “この手の学習プロンプトと画像は、pre-trainモデルではほとんどなかったと思います。ですから、今回、試しにやってみたわりには良い出来だと思います。また、defect modeの出力に結構正解が多かったのも驚きでした。問題はdefect-positionが、全く我々の意図に合わない表現になっています。モデルも我々の意図をよくわかっていないんでしょう。”

C部長 ： “じゃあ、今回は、さらに方法を変えて学習のトライアルをやってみますか？”

QEU:FOUNDER ： “いや・・・。その前に、**少しだけ複雑な推論**をやってみましょう。幸いなことに、小生の手元には前回の学習で得たLORAがあるので、それをインポートして使ってみます。学習は、コストもかかり、失敗の確率も高いので、いきなり学習に進みたくないのです。それでは、推論用のプログラムをドン・・・。”

```python
# ---
import torch
from unsloth import FastVisionModel # FastLanguageModel for LLMs

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit", # Llama 3.2 vision support
    "unsloth/Llama-3.2-11B-Vision-bnb-4bit",
    "unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit", # Can fit in a 80GB card!
    "unsloth/Llama-3.2-90B-Vision-bnb-4bit",

    "unsloth/Pixtral-12B-2409-bnb-4bit",              # Pixtral fits in 16GB!
    "unsloth/Pixtral-12B-Base-2409-bnb-4bit",         # Pixtral base model

    "unsloth/Qwen2-VL-2B-Instruct-bnb-4bit",          # Qwen2 VL support
    "unsloth/Qwen2-VL-7B-Instruct-bnb-4bit",
    "unsloth/Qwen2-VL-72B-Instruct-bnb-4bit",

    "unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit",      # Any Llava variant works!
    "unsloth/llava-1.5-7b-hf-bnb-4bit",
] # More models at https://huggingface.co/unsloth

# ---
# 手元にあるLORAを使ってみる
#str_model_name = "unsloth/Llama-3.2-11B-Vision-Instruct"
str_model_name = "drive/MyDrive/Unsloth_VLM_1125/checkpoint-20"
model, tokenizer = FastVisionModel.from_pretrained(
    str_model_name,
    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for long context
)

```

D先生 ： “たしかに、前回に作成した**LORAアダプタ**を使っていますね。コードのつづきを見てみましょう。”

```python
# ----
import numpy as np

# ----
# reading_pic
mx_image_DST = [["NA"]*2 for i in range(10)]
mx_image_RAW = [["NA"]*2 for i in range(10)]
# ---
# DST image - MATRIX
mx_image_DST[0] = ["combined_image_simple_no193.jpg", "X:1, Y:U"]
mx_image_DST[1] = ["combined_image_simple_no194.jpg", "X:3, Y:U"]
mx_image_DST[2] = ["combined_image_simple_no196.jpg", "X:5, Y:U"]
mx_image_DST[3] = ["combined_image_simple_no200.jpg", "X:7, Y:U"]
mx_image_DST[4] = ["combined_image_simple_no201.jpg", "X:3, Y:C"]
mx_image_DST[5] = ["combined_image_simple_no203.jpg", "X:5, Y:C"]
mx_image_DST[6] = ["combined_image_simple_no206.jpg", "X:7, Y:C"]
mx_image_DST[7] = ["combined_image_simple_no208.jpg", "X:3, Y:D"]
mx_image_DST[8] = ["combined_image_simple_no209.jpg", "X:5, Y:D"]
mx_image_DST[9] = ["combined_image_simple_no212.jpg", "X:7, Y:D"]
mx_image_DST = np.array(mx_image_DST)
#mx_image_DST

# ---
# RAW image - MATRIX
mx_image_RAW[0] = ["combined_image_simple_no192.jpg", "X:4, Y:U"]
mx_image_RAW[1] = ["combined_image_simple_no202.jpg", "X:7, Y:C"]
mx_image_RAW[2] = ["combined_image_simple_no204.jpg", "X:8, Y:D"]
mx_image_RAW[3] = ["combined_image_simple_no205.jpg", "X:1, Y:D"]
mx_image_RAW[4] = ["combined_image_simple_no207.jpg", "X:9, Y:U"]
mx_image_RAW[5] = ["combined_image_simple_no210.jpg", "X:5, Y:C"]
mx_image_RAW[6] = ["combined_image_simple_no211.jpg", "X:3, Y:D"]
mx_image_RAW[7] = ["combined_image_simple_no213.jpg", "X:1, Y:C"]
mx_image_RAW[8] = ["combined_image_simple_no215.jpg", "X:2, Y:C"]
mx_image_RAW[9] = ["combined_image_simple_no217.jpg", "X:3, Y:C"]
mx_image_RAW = np.array(mx_image_RAW)
mx_image_RAW

```

QEU:FOUNDER ： “ここが、今回のプログラムのわかりにくいところです。今回は、DST(NSOARTC処理)とRAWの2種類の画像を10枚づつ用意しています。さらに、今回は、特別に**「赤星つき画像」も準備しました**。”

![imageSMNW3-3-2](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-2.jpg)

D先生 ： “なるほど、すべての画像に赤い星がついています。なになに・・・？タイトルは、**「1SHOT用画像」**か・・・。なるほど、これから1SHOTプロンプトに改造するんですね。・・・でも、VRAMの画像付きプロンプトで1SHOTプロンプトを生成できるのかなぁ・・・。”

QEU:FOUNDER ： “だから、最初にテストとして、今回のように推論部分のみを動かしてみたかったの・・・。さて、プログラムをつづけましょう。”

```python
# -----
# VLM Inference
#import numpy as np
import random
from PIL import Image
from IPython.display import Markdown, display
import matplotlib.pyplot as plt

# -----
# 作画関数(for 3 pics)
def draw_three_pic(arr_image, arr_title):
    # グラフ化
    fig = plt.figure(figsize=(12,10))
    fig.tight_layout(rect=[0,0,0.4,0.4])
    # ---
    plt.subplot(2,2,1)
    plt.title(arr_title[0])
    plt.imshow(arr_image[0])
    # ---
    plt.subplot(2,2,2)
    plt.title(arr_title[1])
    plt.imshow(arr_image[1])
    # ---
    plt.subplot(2,2,3)
    plt.title(arr_title[2])
    plt.imshow(arr_image[2])
    # 画像の表示
    plt.show()

# ---
def generate_index():
    # ---
    # index for Image1
    list_range = list(range(10))
    j_img1 = random.choice(list_range)
    # ---
    # index for Image1
    j_img2 = random.choice(list_range)

    return j_img1, j_img2

# ---
def read_imagefile(mx_image_DST, mx_image_RAW, dataset_temp, icount):
    # ---
    # ランダムイメージを選択する
    str_type = dataset_temp[icount]["type"]
    j_img1, j_img2 = generate_index()
    # ---
    if str_type == 'CYLN_DST':
        str_dir1 = "drive/MyDrive/passed_image/DST_STAR/"
        filename1 = mx_image_DST[j_img1, 0]
        image1 = Image.open(str_dir1 + filename1) 
        str_adress = mx_image_DST[j_img1, 1]
    else:
        str_dir1 = "drive/MyDrive/passed_image/RAW_STAR/"
        filename1 = mx_image_RAW[j_img1, 0]
        image1 = Image.open(str_dir1 + filename1).convert("RGB")
        str_adress = mx_image_RAW[j_img1, 1]
    # ---   
    if str_type == 'CYLN_DST':
        str_dir2 = "drive/MyDrive/passed_image/DST/"
        filename2 = mx_image_DST[j_img2, 0]
        image2 = Image.open(str_dir2 + filename2)         
    else:
        str_dir2 = "drive/MyDrive/passed_image/RAW/"
        filename2 = mx_image_RAW[j_img2, 0]
        image2 = Image.open(str_dir2 + filename2).convert("RGB")
    # ---
    image3 = dataset_temp[icount]["image"]
    # ---
    #print("--- IMAGE(1) ---")
    # 画像をarrayに変換
    #img_measure1 = np.asarray(image1)
    str_title1 = f"ADDRESS: {str_adress}"
    # ---
    #print("--- IMAGE(2) ---")
    # 画像をarrayに変換
    #img_measure2 = np.asarray(image2)
    str_title2 = f"Image2: {filename2}"
    # ---
    #print("--- IMAGE(3) ---")
    # 画像をarrayに変換
    #img_measure3 = np.asarray(image3)
    str_title3 = f"Image3 - DATA NO:{icount}"
    # ---
    arr_image = [image1, image2, image3]
    arr_title = [str_title1, str_title2, str_title3]

    return arr_image, arr_title, str_adress

# ---
# データ番号(icount)を指定すること
icount = 0
arr_image, arr_title, str_adress = read_imagefile(mx_image_DST, mx_image_RAW, dataset_test, icount)
# 画像を表示する
draw_three_pic(arr_image, arr_title)
print("--- str_adress ---")
print(str_adress)

```

QEU:FOUNDER ： “ここでは、3枚の画像を出力しています。”

**（icount = 0）**

![imageSMNW3-3-3](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-3.jpg)

D先生 ： “なるほど。最初のプロンプトは、赤い星の位置だけを検出したいのですね。そして、次のプロンプト・・・は・・・。”

```python
# ---
instruction_STAR = "You are a visual inspector who find defect on a product. Image1 shows a group of pins (cylinders) on the product. This image shows pins from above. There are alphanumeric char-acters located above and left of the product. These alphanumeric characters are used to generate ad-dress where the defect is located. Image1 has a red star as defect. You should specify a location of the red star. After your inspection, you should describe the type of image, number of pins, and position of the red star."
# ---
instruction_DST = "You are a visual inspector who judges whether a product is passed or defective. Both image2 and image3 show a group of pins (cylinders) on the product. These images are top view of the pins. The number and arrangement of pins in image2 and image3 are the same. First, you should confirm that number and arrangement of pins in image2 and image3 must be the same. Then, you should count the quantity of pins in image3. Image2 is an image of passed product. You should compare all of pins on the product in image2 and image3 one by one. If you found a significant dif-ference on the pins, it is defined as defect. After visual inspection, you should describe type of image, quantity of pins, defect mode, and address of defect. You should understand that if you find a solid white circle, it is likely to be a defect."
# ---
instruction_RAW = "You are a visual inspector who judges whether a product is passed or defective. Both image2 and image3 show a group of pins (cylinders) on the product. These images are top view of the pins. The number and arrangement of pins in image2 and image3 are the same. First, you should confirm that number and arrangement of pins in image2 and image3 must be the same. Then, you should count the quantity of pins in image3. Image2 is an image of passed product. You should compare all of pins on the product in image2 and image3 one by one. If you found a significant dif-ference on the pins, it is defined as defect. After visual inspection, you should describe type of image, quantity of pins, defect mode, and address of defect."
#print(instruction_RAW)

# ---
def convert_to_message(sample, str_adress):
    # ----
    str_dir = sample["dir"]
    str_position = sample["position"]
    str_type = sample["type"]
    str_bbox = sample["bbox_str"]
    # ----
    instruction = "NA"
    if str_type == 'CYLN_DST':
        instruction = instruction_DST
    else:
        instruction = instruction_RAW
    # ----
    str_1shot = f"- image type->{str_type}\n- number of pins->27 pins\n- defect position->{str_adress}"
    # ----
    messages = [
        { "role": "user",
          "content" : [
            {"type" : "text",  "text"  : instruction_STAR},
            {"type" : "image", "image" : "image1"} ]
        },
        { "role": "assistant",
          "content" : [
            {"type" : "text",  "text"  : str_1shot},]
        },
        { "role": "user",
          "content" : [
            {"type" : "text",  "text"  : instruction},
            {"type" : "image", "image" : "image2"},
            {"type" : "image", "image" : "image3"} ]
        }
    ]
    # ----
    index_bbox = str_bbox.index("<loc")
    str_defect = str_bbox[:index_bbox]
    str_defective = "passed"
    if str_defect != "passed":
        str_defective = "defective"
    # ----
    str_answer = f"- image type->{str_type}\n- number of pins->27 pins\n- your judgement->{str_defective}\n- defect mode->{str_defect}\n- defect position->{str_position}"

    return messages, instruction, str_1shot, str_answer

# ---
# INSTRUCTION を消してしまう
def eliminate_prompt(response):
    # ---
    split_string = '<|start_header_id|>assistant<|end_header_id|>\n\n'
    index_response = response[0].index(split_string)
    cut_response = response[0][index_response+len(split_string):]
    cut_response = cut_response.replace("<|eot_id|>","")
    #print(cut_response)
    # ---
    str_start = "<|start_header_id|>user<|end_header_id|>"
    index_pick_start = cut_response.index(str_start)
    #print(index_pick_start)
    # ---
    str_end = "<|start_header_id|>assistant<|end_header_id|>"
    index_pick_end = cut_response.index(str_end)
    #print(index_pick_end)
    # ---
    str_pick = cut_response[index_pick_start:index_pick_end] + str_end
    #print(str_pick)
    # ---
    out_response = cut_response.replace(str_pick,"\n+--+--+")
    
    return out_response

# ---
# メッセージを生成する
sample = dataset_test[icount]
message, instruction, str_1shot, str_answer = convert_to_message(sample, str_adress)
print("--- message ---")
print(message)
print("--- instruction_STAR ---")
print(instruction_STAR)
print("--- str_1shot ---")
print(str_1shot)
print("--- instruction ---")
print(instruction)
print("--- str_answer ---")
print(str_answer)

# ---
FastVisionModel.for_inference(model) # Enable for inference!

# ---
# 3つのイメージを使う
images = arr_image
input_text = tokenizer.apply_chat_template(message, add_generation_prompt = True)
inputs = tokenizer(
    images,
    input_text,
    add_special_tokens = False,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
outputs = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)
response = tokenizer.batch_decode(outputs)
# ---
print("--- INSTRUCTION ---")
display(Markdown(f"<b>{instruction}</b>"))
print("--- AI RESPONSE ---")
display(Markdown(f"<b>{eliminate_prompt(response)}</b>"))
print("--- DATASET ANSWER ---")
display(Markdown(f"<b>{str_answer}</b>"))

```

QEU:FOUNDER ： “前回とほとんど同じです。このようにすれば、VMLモデルも欠陥位置の表現方法を理解するだろうと思ったんです。それでは推論の結果をみてみましょう。”

![imageSMNW3-3-4](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-4.jpg)

C部長： “例によって、あまりうまく行っていないようです。・・・ともあれ、1shot目のプロンプトを見せていただけませんか？”


instruction_STAR = "You are a visual inspector who find defect on a product. Image1 shows a group of pins (cylinders) on the product. This image shows pins from above. There are alphanumeric char-acters located above and left of the product. These alphanumeric characters are used to generate ad-dress where the defect is located. Image1 has a red star as defect. You should specify a location of the red star. After your inspection, you should describe the type of image, number of pins, and position of the red star."

あなたは、製品上の欠陥を検出する目視検査員です。画像1は、製品上のピン（シリンダー）のグループを示しています。この画像は、ピンを上から見たものです。製品画像の上側と左側には英数字が配置されています。これらは、あなたが見つけた欠陥がどの位置に存在するかを示すアドレスを生成するために使います。画像1には、1つの赤い星があります。あなたは、その赤い星の位置を指定してください。検査後、画像の種類、ピンの数、および赤い星の場所を記述する必要があります。


D先生 ： “たしかに、このinstruction_STARは画像上の赤い星の位置を見出すためのプロンプトですね。このプロンプトを前回のプロンプトをつなげると1shotプロントになるのか・・・。”

```python
    # ----
    messages = [
        { "role": "user",
          "content" : [
            {"type" : "text",  "text"  : instruction_STAR},
            {"type" : "image", "image" : "image1"} ]
        },
        { "role": "assistant",
          "content" : [
            {"type" : "text",  "text"  : str_1shot},]
        },
        { "role": "user",
          "content" : [
            {"type" : "text",  "text"  : instruction},
            {"type" : "image", "image" : "image2"},
            {"type" : "image", "image" : "image3"} ]
        }
    ]
```

D先生 ： “なるほどねえ・・・。前回とは比較にならないくらい複雑なプロンプトだ。”

QEU:FOUNDER ： “前回と全く違う構造なので、前回のプロンプトで学習したモデルは通用しないんです。今回のトライアルは、そこらへんの不足を承知でやっているんです。他のテスト・データを入力して、同様にやってみましょう。”

![imageSMNW3-3-5](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-5.jpg)

QEU:FOUNDER ： “さきほどはきちんと説明しなかったのですが、回答の構造はこのよう（↑）になっています。1SHOT目の答えは、プログラムの入力値なので無視してもいいです。これも、出力の構造がこわれていますね。次のケースを見てみましょう。”

![imageSMNW3-3-6](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-6.jpg)

D先生 ： “おっと！やっと欠陥アドレスの表現が良くなりました。もちろん、結果はいまだに間違っていますけどね・・・。やっと、1SHOTプロンプトの効果がでてきました。次のケースをお願いします。”

![imageSMNW3-3-7](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-7.jpg)

C部長： “やっぱり、欠陥アドレスを見ると、1SHOTプロンプトの効果が明確ですね。欠陥モードが壊れてしまいましたが・・・。”

QEU:FOUNDER ： “そこらへんは、再度、モデルを学習して評価するしかないですよ。”

D先生 ： “こんなことになるのなら、今回の推論を飛ばし、ちゃんと学習してから評価してもよかったのに・・・。”

QEU:FOUNDER ： “今回のプログラムを見ればわかるでしょ？推論だけでも、ここまでコードが複雑になってしまいました。次回はプログラムを晒さずにいきたい。そのかわり、今回の1SHOT用プログラムを参考にしてもらいたいのです。”

D先生 ： “まあ、コードが複雑になるのは、わかるわぁ・・・。学習データは全て1SHOTプロンプトにするんですか？そうなると、学習負荷がきつくなるでしょうね。”

QEU:FOUNDER ： “いまのところ具体的な方案は決めていないが、3分の2はZERO-SHOTにして、残りの3分の1だけを1-SHOTにしようとおもいます。”

C部長 ： “これは、大変に複雑な学習プログラムだ・・・。”

QEU:FOUNDER ： “しようがないよ。学習の負担を減らしたい。また、本番環境(production)では推論として**ZERO-SHOTだけを使いたい**んです。次回につづきます。ひょっとしたら、ダメかもよ（笑）。そのときは、別のモデルで遊びましょう。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “ただし、いわゆる「右（政治的右派）」といってもねえ・・・。実際のところ、政治的には間口が広いんですよ。ホレ・・・、「れ」のつく党（↓）の投票動向を見てみ・・・。”

![imageSMNW3-3-8](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-8.jpg)

C部長 : “あらあら・・・。半分以上が・・・。”

QEU:FOUNDER ： “あそこが**「いまいち伸びない」**のは、こういう部分なんでしょうね。あそこが、いわゆる**「わかりやすいリベラル」**であれば、こんなことにはならんはずだが・・・。まあ、結局のところ、皆さま、お忙しくて、そんなに深く考えていないと思うんですよ。例の「パワハラ特区」の例みたいに・・・。”

![imageSMNW3-3-9](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-9.jpg)

C部長 : “あの人たち(↑)に19000人も票が集まるとは、ボクも腰が抜けました。さすが、パワハラ特区・・・。”

![imageSMNW3-3-10](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-10.jpg)

QEU:FOUNDER ： “あの人のパワハラは、良いパワハラ・・・。対するに彼のは悪いパワハラ・・・。こんな区分けではダメでしょう。だから、選挙のテーマが「パワハラがあったか？」になると、メタメタになった。”

![MOVIE1](http://img.youtube.com/vi/rX-7dktNAGE/0.jpg)](http://www.youtube.com/watch?v=rX-7dktNAGE "斎藤元彦・兵庫県知事と「高齢男性」問題。")

C部長 : “パワハラ特区民曰く、**「良いハラスメントは、市民のためのハラスメントだから・・・」**。なんじゃこりゃ！？すべてのハラスメントは生産性を落とし、若者の成長を妨げ、すべての人を貧乏にさせるんです。”

![imageSMNW3-3-11](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-11.jpg)

QEU:FOUNDER ： “おっと！C部長はいいことを言うね。ついでに言えば、あのイケメン政党（れ：↓）も、長期的には名正図の立ち位置を変えるべきではないでしょうか。”

![imageSMNW3-3-12](/2024-11-27-QEUR23_SMNW12/imageSMNW3-3-12.jpg)

C部長 : “へえ・・・。意外でした。FOUNDERは、イケメン（れ）推しだと思っていました。”

QEU:FOUNDER ： “小生の主義としてはイケメンに近いのですが、もし投票になればケースバイケースです。それが**「成熟した政治的な立場のあり方」**だと思うのでね・・・。”
